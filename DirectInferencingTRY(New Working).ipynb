{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ca2a5d",
   "metadata": {},
   "source": [
    "# DIRECT INFERENCING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75809124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6982481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -\n",
      "1 #\n",
      "2 <\n",
      "3 >\n",
      "4 ऀ\n",
      "5 ँ\n",
      "6 ं\n",
      "7 ः\n",
      "8 ऄ\n",
      "9 अ\n",
      "10 आ\n",
      "11 इ\n",
      "12 ई\n",
      "13 उ\n",
      "14 ऊ\n",
      "15 ऋ\n",
      "16 ऌ\n",
      "17 ऍ\n",
      "18 ऎ\n",
      "19 ए\n",
      "20 ऐ\n",
      "21 ऑ\n",
      "22 ऒ\n",
      "23 ओ\n",
      "24 औ\n",
      "25 क\n",
      "26 ख\n",
      "27 ग\n",
      "28 घ\n",
      "29 ङ\n",
      "30 च\n",
      "31 छ\n",
      "32 ज\n",
      "33 झ\n",
      "34 ञ\n",
      "35 ट\n",
      "36 ठ\n",
      "37 ड\n",
      "38 ढ\n",
      "39 ण\n",
      "40 त\n",
      "41 थ\n",
      "42 द\n",
      "43 ध\n",
      "44 न\n",
      "45 ऩ\n",
      "46 प\n",
      "47 फ\n",
      "48 ब\n",
      "49 भ\n",
      "50 म\n",
      "51 य\n",
      "52 र\n",
      "53 ऱ\n",
      "54 ल\n",
      "55 ळ\n",
      "56 ऴ\n",
      "57 व\n",
      "58 श\n",
      "59 ष\n",
      "60 स\n",
      "61 ह\n",
      "62 ऺ\n",
      "63 ऻ\n",
      "64 ़\n",
      "65 ऽ\n",
      "66 ा\n",
      "67 ि\n",
      "68 ी\n",
      "69 ु\n",
      "70 ू\n",
      "71 ृ\n",
      "72 ॄ\n",
      "73 ॅ\n",
      "74 ॆ\n",
      "75 े\n",
      "76 ै\n",
      "77 ॉ\n",
      "78 ॊ\n",
      "79 ो\n",
      "80 ौ\n",
      "81 ्\n",
      "82 ॎ\n",
      "83 ॏ\n",
      "84 ॐ\n",
      "85 ॑\n",
      "86 ॒\n",
      "87 ॓\n",
      "88 ॔\n",
      "89 ॕ\n",
      "90 ॖ\n",
      "91 ॗ\n",
      "92 क़\n",
      "93 ख़\n",
      "94 ग़\n",
      "95 ज़\n",
      "96 ड़\n",
      "97 ढ़\n",
      "98 फ़\n",
      "99 य़\n",
      "100 ॠ\n",
      "101 ॡ\n",
      "102 ॢ\n",
      "103 ॣ\n",
      "104 ।\n",
      "105 ॥\n",
      "106 ०\n",
      "107 १\n",
      "108 २\n",
      "109 ३\n",
      "110 ४\n",
      "111 ५\n",
      "112 ६\n",
      "113 ७\n",
      "114 ८\n",
      "115 ९\n",
      "116 ॰\n",
      "117 ॱ\n",
      "118 ॲ\n",
      "119 ॳ\n",
      "120 ॴ\n",
      "121 ॵ\n",
      "122 ॶ\n",
      "123  \n",
      "124 .\n",
      "125 ,\n",
      "126 ?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
    "        super().__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        x = self.emb(x)\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "class SpeechFeatureEmbedding(layers.Layer):\n",
    "    def __init__(self, num_hid=64, maxlen=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return self.conv3(x)\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    \n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.self_att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.self_dropout = layers.Dropout(0.5)\n",
    "        self.enc_dropout = layers.Dropout(0.1)\n",
    "        self.ffn_dropout = layers.Dropout(0.1)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
    "\n",
    "        This prevents flow of information from future tokens to current token.\n",
    "        1's in the lower triangle, counting from the lower right corner.\n",
    "        \"\"\"\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, enc_out, target):\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
    "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
    "        enc_out = self.enc_att(target_norm, enc_out)\n",
    "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
    "        ffn_out = self.ffn(enc_out_norm)\n",
    "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
    "        return ffn_out_norm\n",
    "\n",
    "class Transformer(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid=64,\n",
    "        num_head=2,\n",
    "        num_feed_forward=128,\n",
    "        source_maxlen=100,\n",
    "        target_maxlen=100,\n",
    "        num_layers_enc=4,\n",
    "        num_layers_dec=1,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_input = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        )\n",
    "\n",
    "        self.encoder = keras.Sequential(\n",
    "            [self.enc_input]\n",
    "            + [\n",
    "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i in range(num_layers_dec):\n",
    "            setattr(\n",
    "                self,\n",
    "                f\"dec_layer_{i}\",\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
    "            )\n",
    "\n",
    "        self.classifier = layers.Dense(num_classes)\n",
    "\n",
    "    def decode(self, enc_out, target):\n",
    "        y = self.dec_input(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
    "        return y\n",
    "\n",
    "    def call(self, inputs):\n",
    "        source = inputs[0]\n",
    "        target = inputs[1]\n",
    "        x = self.encoder(source)\n",
    "        y = self.decode(x, target)\n",
    "        return self.classifier(y)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric]\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self([source, dec_input])\n",
    "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        preds = self([source, dec_input])\n",
    "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
    "        bs = tf.shape(source)[0]\n",
    "        enc = self.encoder(source)\n",
    "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
    "        dec_logits = []\n",
    "        for i in range(self.target_maxlen - 1):\n",
    "            dec_out = self.decode(enc, dec_input)\n",
    "            logits = self.classifier(dec_out)\n",
    "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
    "            dec_logits.append(last_logit)\n",
    "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
    "        return dec_input\n",
    "\n",
    "class VectorizeChar:\n",
    "    def __init__(self, max_len=50):\n",
    "        self.vocab = (\n",
    "            [\"-\", \"#\", \"<\", \">\"]\n",
    "            + [chr(i + 2303) for i in range(1, 120)] \n",
    "            + [\" \", \".\", \",\", \"?\"]\n",
    "        )\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.char_to_idx = {}\n",
    "        for i, ch in enumerate(self.vocab):\n",
    "            self.char_to_idx[ch] = i\n",
    "            print(self.char_to_idx[ch],ch)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = text.lower()\n",
    "        text = text[: self.max_len - 2]\n",
    "        text = \"<\" + text + \">\"\n",
    "        pad_len = self.max_len - len(text)\n",
    "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "#         print(self.vocab)\n",
    "        return self.vocab\n",
    "        \n",
    "\n",
    "\n",
    "max_target_len = 200  # all transcripts in our data are < 200 characters\n",
    "# data = get_data(wavs, id_to_text, max_target_len)\n",
    "# # print(data)\n",
    "vectorizer = VectorizeChar(max_target_len)\n",
    "# print(\"vocab size\", len(vectorizer.get_vocabulary()))\n",
    "\n",
    "# import random\n",
    "# random.shuffle(data)\n",
    "\n",
    "\n",
    "def create_text_ds(data):\n",
    "    texts = [_[\"text\"] for _ in data]\n",
    "    text_ds = [vectorizer(t) for t in texts]\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n",
    "#     print(text_ds)\n",
    "    return text_ds\n",
    "\n",
    "\n",
    "def path_to_audio(path):\n",
    "    # spectrogram using stft\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1)\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n",
    "\n",
    "    x = tf.math.pow(tf.abs(stfts), 0.5)\n",
    "\n",
    "    # normalisation\n",
    "    means = tf.math.reduce_mean(x, 1, keepdims=True)\n",
    "    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n",
    "    x = (x - means) / stddevs\n",
    "    audio_len = tf.shape(x)[0]\n",
    "\n",
    "    # padding to 10 seconds\n",
    "    pad_len = 2754\n",
    "    paddings = tf.constant([[0, pad_len], [0, 0]])\n",
    "    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n",
    "    x = tf.where(tf.math.is_nan(x), 0., x) # get rid of all nan values, avoid \"loss:nan\"\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_audio_ds(data):\n",
    "    flist = [_[\"audio\"] for _ in data]\n",
    "    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n",
    "    audio_ds = audio_ds.map(\n",
    "        path_to_audio, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    return audio_ds\n",
    "\n",
    "\n",
    "def create_tf_dataset(data, bs=4):\n",
    "    audio_ds = create_audio_ds(data)\n",
    "    text_ds = create_text_ds(data)\n",
    "    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n",
    "    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n",
    "    ds = ds.batch(bs)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "class DisplayOutputs(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n",
    "    ):\n",
    "        \"\"\"Displays a batch of outputs after every epoch\n",
    "\n",
    "        Args:\n",
    "            batch: A test batch containing the keys \"source\" and \"target\"\n",
    "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
    "            target_start_token_idx: A start token index in the target vocabulary\n",
    "            target_end_token_idx: An end token index in the target vocabulary\n",
    "        \"\"\"\n",
    "        self.batch = batch\n",
    "        print(batch)\n",
    "        self.target_start_token_idx = target_start_token_idx\n",
    "        self.target_end_token_idx = target_end_token_idx\n",
    "        self.idx_to_char = idx_to_token\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 != 0:\n",
    "            return\n",
    "        source = self.batch[\"source\"]\n",
    "        target = self.batch[\"target\"].numpy()\n",
    "        bs = tf.shape(source)[0]\n",
    "        preds = self.model.generate(source, self.target_start_token_idx)\n",
    "        preds = preds.numpy()\n",
    "        for i in range(bs):\n",
    "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
    "            prediction = \"\"\n",
    "            for idx in preds[i, :]:\n",
    "                prediction += self.idx_to_char[idx]\n",
    "                if idx == self.target_end_token_idx:\n",
    "                    break\n",
    "            print(f\"target:     {target_text.replace('-','')}\")\n",
    "            print(f\"prediction: {prediction}\\n\")\n",
    "\n",
    "model = Transformer(\n",
    "    num_hid=200,\n",
    "    num_head=2,\n",
    "    num_feed_forward=400,\n",
    "    target_maxlen=max_target_len,\n",
    "    num_layers_enc=4,\n",
    "    num_layers_dec=1,\n",
    "    num_classes=127,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52920e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1e1a5afea90>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading one of the saved model\n",
    "model_path = r\".\\models\\smallDatasetCheckpoint-48-0.12\"\n",
    "# model_path = r\".\\models\\smallDatasetCheckpoint-56-0.12\"\n",
    "# model_path = r\".\\models\\smallDatasetCheckpoint-56-0.12\"\n",
    "model.load_weights(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0941bafe",
   "metadata": {},
   "source": [
    "# ###LIVE AUDIO####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b6e5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please uncomment the 2 lines below if the dependencies are not downloaded already.\n",
    "\n",
    "# !pip install sounddevice\n",
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "263369ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##LIVE RECORD TRANSLATION (FOR DEMO)\n",
    "# import required libraries\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "#animation imports\n",
    "import itertools\n",
    "import threading\n",
    "import time\n",
    "import sys\n",
    "import sounddevice\n",
    "\n",
    "def my_rec():\n",
    "    done = False\n",
    "    #here is the animation\n",
    "    def animate():\n",
    "        for c in itertools.cycle(['|', '/', '-', '\\\\']):\n",
    "            if done:\n",
    "                break\n",
    "            sys.stdout.write('\\rRecording ' + c)\n",
    "            sys.stdout.flush()\n",
    "            time.sleep(0.1)\n",
    "        sys.stdout.write('\\rDone!')\n",
    "\n",
    "    fs= 16000  \n",
    "    second = 4\n",
    "    t = threading.Thread(target=animate)\n",
    "    t.start()\n",
    "    #long process here\n",
    "    print(\"Please Speak in Nepali\")\n",
    "    record_voice = sounddevice.rec( int ( second * fs ) , samplerate = fs , channels = 1, dtype='int16' )\n",
    "    sounddevice.wait()\n",
    "    time.sleep(1)\n",
    "    done = True\n",
    "    aud_path=r\"./live_audrec/Audiotest.wav\"\n",
    "    write(aud_path,fs,record_voice)\n",
    "    print(\"Saved in REC folder\")\n",
    "\n",
    "#Predictor functions\n",
    "\n",
    "def single_model_predict():\n",
    "    #Prediction of the recorded audio.\n",
    "    print(\"Loading the transcription\")\n",
    "    path=r\"./live_audrec/Audiotest.wav\"\n",
    "\n",
    "    # print(path)\n",
    "    x = path_to_audio(path)\n",
    "    #print(x)\n",
    "    x = tf.expand_dims(x, axis=0)\n",
    "    # print(x.shape)\n",
    "    idx_to_char = vectorizer.get_vocabulary()\n",
    "    preds = model.generate(x, 2)\n",
    "    preds = preds.numpy()\n",
    "    bs = tf.shape(x)[0]\n",
    "    for i in range(bs):\n",
    "        prediction = \"\"\n",
    "        for idx in preds[i, :]:\n",
    "            prediction += idx_to_char[idx]\n",
    "            if idx == 3:\n",
    "                break\n",
    "\n",
    "    print(\"prediction: \", prediction)\n",
    "\n",
    "\n",
    "def all_model_predict(models_paths):\n",
    "        #Prediction of the recorded audio.\n",
    "    \n",
    "    path=r\"./live_audrec/Audiotest.wav\"\n",
    "    mod_no=0\n",
    "    for model_path in models_paths:\n",
    "            # print(path)\n",
    "        model.load_weights(model_path)\n",
    "        x = path_to_audio(path)\n",
    "        #print(x)\n",
    "        x = tf.expand_dims(x, axis=0)\n",
    "        # print(x.shape)\n",
    "        idx_to_char = vectorizer.get_vocabulary()\n",
    "        preds = model.generate(x, 2)\n",
    "        preds = preds.numpy()\n",
    "        bs = tf.shape(x)[0]\n",
    "        for i in range(bs):\n",
    "            prediction = \"\"\n",
    "            for idx in preds[i, :]:\n",
    "                prediction += idx_to_char[idx]\n",
    "                if idx == 3:\n",
    "                    break\n",
    "        mod_no=mod_no+1\n",
    "\n",
    "        print(\"prediction from model{}: \".format(mod_no), prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac26832d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording |Please Speak in Nepali\n",
      "Recording /Saved in REC folder\n",
      "Done!"
     ]
    }
   ],
   "source": [
    "#calling my_rec() function\n",
    "my_rec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e906e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the transcription\n",
      "prediction:  <मेरो नाम सिसिर पौडेल हो>\n"
     ]
    }
   ],
   "source": [
    "#calling single_model_predict() function\n",
    "single_model_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08c8a99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the transcriptions\n",
      "prediction from model1:  <मेरो नाम सिसिर पौडेल हो>\n",
      "prediction from model2:  <मेरो नाम सिसिट पौडेल हो>\n",
      "prediction from model3:  <मेरो नाम सिसिर पौडेल हो>\n"
     ]
    }
   ],
   "source": [
    "# Loading all of the three saved models\n",
    "model1_path = r\".\\models\\smallDatasetCheckpoint-48-0.12\" # trained for 48 epochs\n",
    "model2_path = r\".\\models\\smallDatasetCheckpoint-56-0.12\" # trained for 56 epochs\n",
    "model3_path = r\".\\models\\smallDatasetCheckpoint-57-0.12\" # trained for 57 epochs\n",
    "\n",
    "models_paths=[model1_path,model2_path,model3_path]\n",
    "\n",
    "print(\"Loading the transcriptions\")\n",
    "# call each model in sequence\n",
    "all_model_predict(models_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a65f67",
   "metadata": {},
   "source": [
    "# New WER calcualtion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
