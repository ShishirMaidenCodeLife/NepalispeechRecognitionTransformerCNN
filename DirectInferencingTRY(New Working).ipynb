{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ca2a5d",
   "metadata": {},
   "source": [
    "# DIRECT INFERENCING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "001c4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6982481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
    "        super().__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        x = self.emb(x)\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "class SpeechFeatureEmbedding(layers.Layer):\n",
    "    def __init__(self, num_hid=64, maxlen=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return self.conv3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9760be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb8fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.self_att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.self_dropout = layers.Dropout(0.5)\n",
    "        self.enc_dropout = layers.Dropout(0.1)\n",
    "        self.ffn_dropout = layers.Dropout(0.1)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
    "\n",
    "        This prevents flow of information from future tokens to current token.\n",
    "        1's in the lower triangle, counting from the lower right corner.\n",
    "        \"\"\"\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, enc_out, target):\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
    "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
    "        enc_out = self.enc_att(target_norm, enc_out)\n",
    "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
    "        ffn_out = self.ffn(enc_out_norm)\n",
    "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
    "        return ffn_out_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e1b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid=64,\n",
    "        num_head=2,\n",
    "        num_feed_forward=128,\n",
    "        source_maxlen=100,\n",
    "        target_maxlen=100,\n",
    "        num_layers_enc=4,\n",
    "        num_layers_dec=1,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_input = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        )\n",
    "\n",
    "        self.encoder = keras.Sequential(\n",
    "            [self.enc_input]\n",
    "            + [\n",
    "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i in range(num_layers_dec):\n",
    "            setattr(\n",
    "                self,\n",
    "                f\"dec_layer_{i}\",\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
    "            )\n",
    "\n",
    "        self.classifier = layers.Dense(num_classes)\n",
    "\n",
    "    def decode(self, enc_out, target):\n",
    "        y = self.dec_input(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
    "        return y\n",
    "\n",
    "    def call(self, inputs):\n",
    "        source = inputs[0]\n",
    "        target = inputs[1]\n",
    "        x = self.encoder(source)\n",
    "        y = self.decode(x, target)\n",
    "        return self.classifier(y)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric]\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self([source, dec_input])\n",
    "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        preds = self([source, dec_input])\n",
    "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
    "        bs = tf.shape(source)[0]\n",
    "        enc = self.encoder(source)\n",
    "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
    "        dec_logits = []\n",
    "        for i in range(self.target_maxlen - 1):\n",
    "            dec_out = self.decode(enc, dec_input)\n",
    "            logits = self.classifier(dec_out)\n",
    "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
    "            dec_logits.append(last_logit)\n",
    "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
    "        return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dd2d139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -\n",
      "1 #\n",
      "2 <\n",
      "3 >\n",
      "4 ऀ\n",
      "5 ँ\n",
      "6 ं\n",
      "7 ः\n",
      "8 ऄ\n",
      "9 अ\n",
      "10 आ\n",
      "11 इ\n",
      "12 ई\n",
      "13 उ\n",
      "14 ऊ\n",
      "15 ऋ\n",
      "16 ऌ\n",
      "17 ऍ\n",
      "18 ऎ\n",
      "19 ए\n",
      "20 ऐ\n",
      "21 ऑ\n",
      "22 ऒ\n",
      "23 ओ\n",
      "24 औ\n",
      "25 क\n",
      "26 ख\n",
      "27 ग\n",
      "28 घ\n",
      "29 ङ\n",
      "30 च\n",
      "31 छ\n",
      "32 ज\n",
      "33 झ\n",
      "34 ञ\n",
      "35 ट\n",
      "36 ठ\n",
      "37 ड\n",
      "38 ढ\n",
      "39 ण\n",
      "40 त\n",
      "41 थ\n",
      "42 द\n",
      "43 ध\n",
      "44 न\n",
      "45 ऩ\n",
      "46 प\n",
      "47 फ\n",
      "48 ब\n",
      "49 भ\n",
      "50 म\n",
      "51 य\n",
      "52 र\n",
      "53 ऱ\n",
      "54 ल\n",
      "55 ळ\n",
      "56 ऴ\n",
      "57 व\n",
      "58 श\n",
      "59 ष\n",
      "60 स\n",
      "61 ह\n",
      "62 ऺ\n",
      "63 ऻ\n",
      "64 ़\n",
      "65 ऽ\n",
      "66 ा\n",
      "67 ि\n",
      "68 ी\n",
      "69 ु\n",
      "70 ू\n",
      "71 ृ\n",
      "72 ॄ\n",
      "73 ॅ\n",
      "74 ॆ\n",
      "75 े\n",
      "76 ै\n",
      "77 ॉ\n",
      "78 ॊ\n",
      "79 ो\n",
      "80 ौ\n",
      "81 ्\n",
      "82 ॎ\n",
      "83 ॏ\n",
      "84 ॐ\n",
      "85 ॑\n",
      "86 ॒\n",
      "87 ॓\n",
      "88 ॔\n",
      "89 ॕ\n",
      "90 ॖ\n",
      "91 ॗ\n",
      "92 क़\n",
      "93 ख़\n",
      "94 ग़\n",
      "95 ज़\n",
      "96 ड़\n",
      "97 ढ़\n",
      "98 फ़\n",
      "99 य़\n",
      "100 ॠ\n",
      "101 ॡ\n",
      "102 ॢ\n",
      "103 ॣ\n",
      "104 ।\n",
      "105 ॥\n",
      "106 ०\n",
      "107 १\n",
      "108 २\n",
      "109 ३\n",
      "110 ४\n",
      "111 ५\n",
      "112 ६\n",
      "113 ७\n",
      "114 ८\n",
      "115 ९\n",
      "116 ॰\n",
      "117 ॱ\n",
      "118 ॲ\n",
      "119 ॳ\n",
      "120 ॴ\n",
      "121 ॵ\n",
      "122 ॶ\n",
      "123  \n",
      "124 .\n",
      "125 ,\n",
      "126 ?\n"
     ]
    }
   ],
   "source": [
    "class VectorizeChar:\n",
    "    def __init__(self, max_len=50):\n",
    "        self.vocab = (\n",
    "            [\"-\", \"#\", \"<\", \">\"]\n",
    "            + [chr(i + 2303) for i in range(1, 120)] \n",
    "#             + [chr(i + 96) for i in range(1, 27)]\n",
    "            + [\" \", \".\", \",\", \"?\"]\n",
    "        )\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.char_to_idx = {}\n",
    "        for i, ch in enumerate(self.vocab):\n",
    "            self.char_to_idx[ch] = i\n",
    "            print(self.char_to_idx[ch],ch)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = text.lower()\n",
    "        text = text[: self.max_len - 2]\n",
    "        text = \"<\" + text + \">\"\n",
    "        pad_len = self.max_len - len(text)\n",
    "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "#         print(self.vocab)\n",
    "        return self.vocab\n",
    "        \n",
    "\n",
    "\n",
    "max_target_len = 200  # all transcripts in our data are < 200 characters\n",
    "# data = get_data(wavs, id_to_text, max_target_len)\n",
    "# # print(data)\n",
    "vectorizer = VectorizeChar(max_target_len)\n",
    "# print(\"vocab size\", len(vectorizer.get_vocabulary()))\n",
    "\n",
    "# import random\n",
    "# random.shuffle(data)\n",
    "\n",
    "\n",
    "def create_text_ds(data):\n",
    "    texts = [_[\"text\"] for _ in data]\n",
    "    text_ds = [vectorizer(t) for t in texts]\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n",
    "#     print(text_ds)\n",
    "    return text_ds\n",
    "\n",
    "\n",
    "def path_to_audio(path):\n",
    "    # spectrogram using stft\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1)\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n",
    "\n",
    "    x = tf.math.pow(tf.abs(stfts), 0.5)\n",
    "\n",
    "    # normalisation\n",
    "    means = tf.math.reduce_mean(x, 1, keepdims=True)\n",
    "    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n",
    "    x = (x - means) / stddevs\n",
    "    audio_len = tf.shape(x)[0]\n",
    "\n",
    "    # padding to 10 seconds\n",
    "    pad_len = 2754\n",
    "    paddings = tf.constant([[0, pad_len], [0, 0]])\n",
    "    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n",
    "    x = tf.where(tf.math.is_nan(x), 0., x) # get rid of all nan values, avoid \"loss:nan\"\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_audio_ds(data):\n",
    "    flist = [_[\"audio\"] for _ in data]\n",
    "    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n",
    "    audio_ds = audio_ds.map(\n",
    "        path_to_audio, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    return audio_ds\n",
    "\n",
    "\n",
    "def create_tf_dataset(data, bs=4):\n",
    "    audio_ds = create_audio_ds(data)\n",
    "    text_ds = create_text_ds(data)\n",
    "    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n",
    "    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n",
    "    ds = ds.batch(bs)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# split = int(len(data) * 0.99) #spliting total 157905 data into 99:1. or \n",
    "# train_data = data[:split] #99% of total data ie: 156325\n",
    "# print(train_data[0])\n",
    "# test_data = data[split:]  #1580\n",
    "# # print(test_data[0])\n",
    "# ds = create_tf_dataset(train_data, bs=64) #2443 ie: total train_data divided by batch size.\n",
    "# # print(ds)\n",
    "# val_ds = create_tf_dataset(test_data, bs=4) #395 ie: total test_data divided by batch size 4\n",
    "# print(\"the tensor of validaiton dataset:\\n\")\n",
    "# # print(val_ds)#tensor structure of source and target data\n",
    "\n",
    "\n",
    "# #Spliting Train and Val+Train data\n",
    "# split = int(len(data) * 0.96)\n",
    "# train_data = data[:split]\n",
    "# TestnVal_data = data[split:]\n",
    "# #Now Splitting test and validation data From Val+Train\n",
    "# split2 = int(len(TestnVal_data)*0.6)\n",
    "# test_data = TestnVal_data[:split2]\n",
    "# FinalVal_data = TestnVal_data[split2:]\n",
    "# ds = create_tf_dataset(train_data, bs=64)\n",
    "# val_ds = create_tf_dataset(test_data, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c223b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayOutputs(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n",
    "    ):\n",
    "        \"\"\"Displays a batch of outputs after every epoch\n",
    "\n",
    "        Args:\n",
    "            batch: A test batch containing the keys \"source\" and \"target\"\n",
    "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
    "            target_start_token_idx: A start token index in the target vocabulary\n",
    "            target_end_token_idx: An end token index in the target vocabulary\n",
    "        \"\"\"\n",
    "        self.batch = batch\n",
    "        print(batch)\n",
    "        self.target_start_token_idx = target_start_token_idx\n",
    "        self.target_end_token_idx = target_end_token_idx\n",
    "        self.idx_to_char = idx_to_token\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 != 0:\n",
    "            return\n",
    "        source = self.batch[\"source\"]\n",
    "        target = self.batch[\"target\"].numpy()\n",
    "        bs = tf.shape(source)[0]\n",
    "        preds = self.model.generate(source, self.target_start_token_idx)\n",
    "        preds = preds.numpy()\n",
    "        for i in range(bs):\n",
    "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
    "            prediction = \"\"\n",
    "            for idx in preds[i, :]:\n",
    "                prediction += self.idx_to_char[idx]\n",
    "                if idx == self.target_end_token_idx:\n",
    "                    break\n",
    "            print(f\"target:     {target_text.replace('-','')}\")\n",
    "            print(f\"prediction: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4f9bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    num_hid=200,\n",
    "    num_head=2,\n",
    "    num_feed_forward=400,\n",
    "    target_maxlen=max_target_len,\n",
    "    num_layers_enc=4,\n",
    "    num_layers_dec=1,\n",
    "    num_classes=127,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4fbd26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1c7325d05b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = r\"D:\\\\shishir_ml\\\\Untitled Folder\\\\TestModeltwo\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec50c3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1c75e532560>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = r\"D:\\meroDs\\MyNewAudioDatasetsTHESIS\\MyFinalDataset\\TrainedModels\\myDatasetCheckpoint-30-0.31\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "738ad830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1951127d960>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = r\"F:\\\\TRAINEDModelSfromActiveLearningLabSharing\\\\Untitled Folder\\\\ModelFor99isTo1\\\\Checkpoints for 99 is to 1\\\\smallDatasetCheckpoint-57-0.12\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2579f7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x19538597580>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this model is fine\n",
    "model_path = r\"F:\\\\TRAINEDModelSfromActiveLearningLabSharing\\\\Untitled Folder\\\\ModelFor99isTo1\\\\Checkpoints for 99 is to 1\\\\smallDatasetCheckpoint-56-0.12\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52920e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1aac21215a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New 2023 Model\n",
    "# model_path = r\"F:\\\\TRAINEDModelSfromActiveLearningLabSharing\\\\Untitled Folder\\\\ModelFor99isTo1\\\\Checkpoints for 99 is to 1\\\\smallDatasetCheckpoint-48-0.12\"\n",
    "# model.load_weights(model_path)\n",
    "\n",
    "model_path = r\"D:\\New folder\\TOBECOPIEDTONEWLAPTOP\\THESIS Important\\THESIS CODING\\ActivelearningLabsharing\\bestMODELS\\smallDatasetCheckpoint-48-0.12\"\n",
    "model.load_weights(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cff26aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the transcription\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "{{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} NewRandomAccessFile failed to Create/Open: D:\\shishir_ml\\Untitled Folder/Rec/Audiotest.wav : The system cannot find the path specified.\r\n; No such process [Op:ReadFile]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mshishir_ml\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUntitled Folder/Rec/Audiotest.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print(path)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mpath_to_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#print(x)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 49\u001b[0m, in \u001b[0;36mpath_to_audio\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpath_to_audio\u001b[39m(path):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# spectrogram using stft\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     audio, _ \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mdecode_wav(audio, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     51\u001b[0m     audio \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(audio, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\io_ops.py:133\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(filename, name)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.read_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.read_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_file\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_file\u001b[39m(filename, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Reads the contents of file.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m  This operation returns a tensor with the entire contents of the input\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    A tensor of dtype \"string\", with the file contents.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_io_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py:581\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(filename, name)\u001b[0m\n\u001b[0;32m    579\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_file_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m    584\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py:604\u001b[0m, in \u001b[0;36mread_file_eager_fallback\u001b[1;34m(filename, name, ctx)\u001b[0m\n\u001b[0;32m    602\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [filename]\n\u001b[0;32m    603\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 604\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReadFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[0;32m    607\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[0;32m    608\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadFile\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mNotFoundError\u001b[0m: {{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} NewRandomAccessFile failed to Create/Open: D:\\shishir_ml\\Untitled Folder/Rec/Audiotest.wav : The system cannot find the path specified.\r\n; No such process [Op:ReadFile]"
     ]
    }
   ],
   "source": [
    "#PREDICTING ANY AUDIO\n",
    "print(\"Loading the transcription\")\n",
    "path=\"D:\\\\shishir_ml\\\\Untitled Folder/Rec/Audiotest.wav\"\n",
    "\n",
    "\n",
    "# print(path)\n",
    "\n",
    "x = path_to_audio(path)\n",
    "#print(x)\n",
    "x = tf.expand_dims(x, axis=0)\n",
    "# print(x.shape)\n",
    "idx_to_char = vectorizer.get_vocabulary()\n",
    "preds = model.generate(x, 2)\n",
    "preds = preds.numpy()\n",
    "bs = tf.shape(x)[0]\n",
    "for i in range(bs):\n",
    "    prediction = \"\"\n",
    "    for idx in preds[i, :]:\n",
    "        prediction += idx_to_char[idx]\n",
    "        if idx == 3:\n",
    "            break\n",
    "            \n",
    "print(\"prediction: \", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e239c083",
   "metadata": {},
   "source": [
    "# SKIP FROM HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9cf612",
   "metadata": {},
   "source": [
    "# OLD CER CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "271aea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wer(r, h):\n",
    "    \"\"\"\n",
    "    Calculation of WER with Levenshtein distance.\n",
    "\n",
    "    Works only for iterables up to 254 elements (uint8).\n",
    "    O(nm) time ans space complexity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    r : list\n",
    "    h : list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> wer(\"who is there\".split(), \"is there\".split())\n",
    "    1\n",
    "    >>> wer(\"who is there\".split(), \"\".split())\n",
    "    3\n",
    "    >>> wer(\"\".split(), \"who is there\".split())\n",
    "    3\n",
    "    \"\"\"\n",
    "    # initialisation\n",
    "    import numpy\n",
    "\n",
    "    d = numpy.zeros((len(r) + 1) * (len(h) + 1), dtype=numpy.uint8)\n",
    "    d = d.reshape((len(r) + 1, len(h) + 1))\n",
    "    for i in range(len(r) + 1):\n",
    "        for j in range(len(h) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(r) + 1):\n",
    "        for j in range(1, len(h) + 1):\n",
    "            if r[i - 1] == h[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "\n",
    "    return d[len(r)][len(h)]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "\n",
    "    doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006796be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTING FROM TEST DATA Alredy downsampled\n",
    "# PREDICTION and WER EVALUATION FOR ANYDATA.\n",
    "myfile = open('D:\\\\shishir_ml\\\\Untitled Folder\\datasets\\LJSpeech\\\\utt_spk_text.tsv', 'r',encoding = 'utf-8')\n",
    "Lines = myfile.readlines()\n",
    "\n",
    "count = 0\n",
    "my_list=[]\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    count += 1\n",
    "    # wordarr = line.split()\n",
    "    arr = line.split(\"\\t\")\n",
    "    my_list.append(arr)\n",
    "    # print (arr)       \n",
    "# audio_iddd=arr[0]\n",
    "# audio_text=arr[2]\n",
    "    # import required module\n",
    "from playsound import playsound\n",
    "\n",
    "# path=\"D:\\\\shishir_ml\\\\Untitled Folder\\\\tess_1.wav\"\n",
    "\n",
    "wer_list=[]\n",
    "for xz in range(157725,157730):\n",
    "# for xz in range(1):\n",
    "\n",
    "    path=\"D:\\\\shishir_ml\\\\Untitled Folder\\datasets\\LJSpeech/wavs/\" + my_list[xz][0]+\".wav\" #the audio\n",
    "\n",
    "\n",
    "    # for playing note.wav file\n",
    "    print(\"Playing the audio file\")\n",
    "#     playsound(path)\n",
    "    \n",
    "    referencetxtt=my_list[xz][2]\n",
    "    x = path_to_audio(path)\n",
    "    #print(x)\n",
    "    x = tf.expand_dims(x, axis=0)\n",
    "    # print(x.shape)\n",
    "    idx_to_char = vectorizer.get_vocabulary()\n",
    "    preds = model.generate(x, 2)\n",
    "    preds = preds.numpy()\n",
    "    bs = tf.shape(x)[0]\n",
    "    for i in range(bs):\n",
    "        prediction = \"\"\n",
    "        for idx in preds[i, :]:\n",
    "            prediction += idx_to_char[idx]\n",
    "            if idx == 3:\n",
    "                break\n",
    "#     print(\"reference:\", referencetxtt)\n",
    "#     print(\"prediction: \", prediction,\"\\n\")\n",
    "    \n",
    "    ref= referencetxtt\n",
    "    hyp= prediction\n",
    "    new=ref.replace(\"\\n\", \"\")\n",
    "    new_ref=\"<\"+new+\">\"\n",
    "    print(\"Reference:\",new_ref)\n",
    "    print(\"Prediction:\",prediction)\n",
    "    werr=wer(new_ref,hyp)\n",
    "    print(\"CER:\")\n",
    "    print(werr)\n",
    "    wer_list.append(werr)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(wer_list)\n",
    "print(\"the avg cer is:\")\n",
    "Avg_wer=sum(wer_list) / len(wer_list)\n",
    "print(Avg_wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef417ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICING FOR NON- DOWNSAMPLED DATASET\n",
    "##Audio TRANSLATION\n",
    "# import required libraries\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0453edf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <दीपा धामीको जन्म सुदूरपश्चिम नेपालको बझाङ जिल्लामा भएको हो>\n",
      "Prediction: <दिपादन सुदूरमा सुदूर्व स्वीलामा भएको हो>\n",
      "['<दीपा', 'धामीको', 'जन्म', 'सुदूरपश्चिम', 'नेपालको', 'बझाङ', 'जिल्लामा', 'भएको', 'हो>'] ['<दिपादन', 'सुदूरमा', 'सुदूर्व', 'स्वीलामा', 'भएको', 'हो>']\n",
      "OP\tREF\tHYP\n",
      "DEL\t<दीपा\t****\n",
      "DEL\tधामीको\t****\n",
      "DEL\tजन्म\t****\n",
      "SUB\tसुदूरपश्चिम\t<दिपादन\n",
      "SUB\tनेपालको\tसुदूरमा\n",
      "SUB\tबझाङ\tसुदूर्व\n",
      "SUB\tजिल्लामा\tस्वीलामा\n",
      "OK\tभएको\tभएको\n",
      "OK\tहो>\tहो>\n",
      "#cor 2\n",
      "#sub 4\n",
      "#del 3\n",
      "#ins 0\n",
      "wer: 0.778\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_ref\u001b[38;5;241m.\u001b[39msplit(),hyp\u001b[38;5;241m.\u001b[39msplit())\n\u001b[0;32m     64\u001b[0m totError\u001b[38;5;241m=\u001b[39mwer(new_ref,hyp)\n\u001b[1;32m---> 65\u001b[0m totworderror\u001b[38;5;241m=\u001b[39m\u001b[43mwer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_ref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhyp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotalCharError:\u001b[39m\u001b[38;5;124m\"\u001b[39m,totError)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotalWordError:\u001b[39m\u001b[38;5;124m\"\u001b[39m,totworderror)\n",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36mwer\u001b[1;34m(ref, hyp, debug)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwer\u001b[39m(ref, hyp ,debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m----> 5\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m()\n\u001b[0;32m      6\u001b[0m     h \u001b[38;5;241m=\u001b[39m hyp\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#costs will holds the costs, like in the Levenshtein distance algorithm\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "#TESTING FROM ANOTHER DATASETS ( NEEDS MORE TRAINING EPOCHS BEFORE GOOD DEMO)\n",
    "myfile = open(r'D:\\shishir_ml\\Untitled Folder\\my_Test_datasetss\\line_index.tsv', 'r',encoding = 'utf-8')\n",
    "Lines = myfile.readlines()\n",
    "from pydub import AudioSegment as am\n",
    "\n",
    "count = 0\n",
    "my_lisst=[]\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    count += 1\n",
    "    # wordarr = line.split()\n",
    "    arr = line.split(\"\\t\")\n",
    "    my_lisst.append(arr)\n",
    "    # print (arr)       \n",
    "# audio_iddd=arr[0]\n",
    "# audio_text=arr[2]\n",
    "    # import required module\n",
    "\n",
    "from playsound import playsound\n",
    "\n",
    "# path=\"D:\\\\shishir_ml\\\\Untitled Folder\\\\tess_1.wav\"\n",
    "\n",
    "weer_list=[]\n",
    "# for xxz in range(2063):\n",
    "for xxz in range(10):\n",
    "\n",
    "    filepath= r\"D:\\shishir_ml\\Untitled Folder\\my_Test_datasetss\\wavs/\" + my_lisst[xxz][0]+\".wav\" #the audio\n",
    "#     print(\"Playing the audio file...\")\n",
    "#     playsound(filepath)\n",
    "    saving_dir=f\"D:\\\\shishir_ml\\\\Untitled Folder\\\\my_Test_datasets\\\\Converted\\\\{xxz}.wav\"\n",
    "    sound = am.from_file(filepath, format='wav')\n",
    "    sound = sound.set_frame_rate(16000)\n",
    "    sound.export(saving_dir, format='wav')\n",
    "    pathh= saving_dir\n",
    "    # for playing note.wav file\n",
    " \n",
    "\n",
    "    referencetxttz=my_lisst[xxz][1]\n",
    "    x = path_to_audio(pathh)\n",
    "    #print(x)\n",
    "    x = tf.expand_dims(x, axis=0)\n",
    "    # print(x.shape)\n",
    "    idx_to_char = vectorizer.get_vocabulary()\n",
    "    preds = model.generate(x, 2)\n",
    "    preds = preds.numpy()\n",
    "    bs = tf.shape(x)[0]\n",
    "    for i in range(bs):\n",
    "        prediction = \"\"\n",
    "        for idx in preds[i, :]:\n",
    "            prediction += idx_to_char[idx]\n",
    "            if idx == 3:\n",
    "                break\n",
    "#     print(\"reference:\", referencetxtt)\n",
    "#     print(\"prediction: \", prediction,\"\\n\")\n",
    "    \n",
    "    ref= referencetxttz\n",
    "    hyp= prediction\n",
    "    new=ref.replace(\"\\n\", \"\")\n",
    "    new_ref=\"<\"+new+\">\"\n",
    "    print(\"Reference:\",new_ref)\n",
    "    print(\"Prediction:\",prediction)\n",
    "    \n",
    "    print(new_ref.split(),hyp.split())\n",
    "    totError=wer(new_ref,hyp)\n",
    "    totworderror=wer(new_ref.split(),hyp.split())\n",
    "    print(\"totalCharError:\",totError)\n",
    "    print(\"totalWordError:\",totworderror)\n",
    "    weerr=(totError/len(new_ref))*100\n",
    "    wwerrr=(totworderror/len(new_ref.split()))*100\n",
    "    \n",
    "#     weerr=wer(new_ref,hyp) \n",
    "    print(\"Sen_no:\",xxz)\n",
    "    print(\"CER:\",weerr,\"%\")\n",
    "    print(\"WER:\",wwerrr,\"%\")\n",
    "    weer_list.append(weerr)\n",
    "    \n",
    "print(weer_list)\n",
    "print(\"the avg cer is:\")\n",
    "Avg_wer=sum(weer_list) / len(weer_list)\n",
    "print(Avg_wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef58a9cf",
   "metadata": {},
   "source": [
    "# CONTInue FROM HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0941bafe",
   "metadata": {},
   "source": [
    "# ###LIVE AUDIO####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b6e5ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.4.6-py3-none-win_amd64.whl (199 kB)\n",
      "                                              0.0/199.7 kB ? eta -:--:--\n",
      "     --                                       10.2/199.7 kB ? eta -:--:--\n",
      "     -----                                 30.7/199.7 kB 325.1 kB/s eta 0:00:01\n",
      "     -----                                 30.7/199.7 kB 325.1 kB/s eta 0:00:01\n",
      "     -----                                 30.7/199.7 kB 325.1 kB/s eta 0:00:01\n",
      "     -----                                 30.7/199.7 kB 325.1 kB/s eta 0:00:01\n",
      "     -----                                 30.7/199.7 kB 325.1 kB/s eta 0:00:01\n",
      "     -----------                           61.4/199.7 kB 181.6 kB/s eta 0:00:01\n",
      "     -----------------                     92.2/199.7 kB 238.1 kB/s eta 0:00:01\n",
      "     --------------------                 112.6/199.7 kB 273.1 kB/s eta 0:00:01\n",
      "     --------------------                 112.6/199.7 kB 273.1 kB/s eta 0:00:01\n",
      "     --------------------                 112.6/199.7 kB 273.1 kB/s eta 0:00:01\n",
      "     ----------------------               122.9/199.7 kB 211.7 kB/s eta 0:00:01\n",
      "     -------------------------            143.4/199.7 kB 229.9 kB/s eta 0:00:01\n",
      "     ---------------------------          153.6/199.7 kB 223.7 kB/s eta 0:00:01\n",
      "     -------------------------------      174.1/199.7 kB 243.8 kB/s eta 0:00:01\n",
      "     -------------------------------      174.1/199.7 kB 243.8 kB/s eta 0:00:01\n",
      "     ------------------------------------ 199.7/199.7 kB 247.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\shishir\\miniconda3\\lib\\site-packages (from sounddevice) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shishir\\miniconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Installing collected packages: sounddevice\n",
      "Successfully installed sounddevice-0.4.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sounddevice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cb87b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "##LIVE RECORD TRANSLATION (FOR DEMO)\n",
    "# import required libraries\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "#animation imports\n",
    "import itertools\n",
    "import threading\n",
    "import time\n",
    "import sys\n",
    "import sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cdf05b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "#here is the animation\n",
    "def animate():\n",
    "    for c in itertools.cycle(['|', '/', '-', '\\\\']):\n",
    "        if done:\n",
    "            break\n",
    "        sys.stdout.write('\\rRecording ' + c)\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(0.1)\n",
    "    sys.stdout.write('\\rDone!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a19cd907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!Please Speak in Nepali\n",
      "Saved in REC folder\n"
     ]
    }
   ],
   "source": [
    "fs= 16000  \n",
    "second = 4\n",
    "\n",
    "t = threading.Thread(target=animate)\n",
    "\n",
    "t.start()\n",
    "\n",
    "\n",
    "\n",
    "#long process here\n",
    "print(\"Please Speak in Nepali\")\n",
    "record_voice = sounddevice.rec( int ( second * fs ) , samplerate = fs , channels = 1, dtype='int16' )\n",
    "sounddevice.wait()\n",
    "\n",
    "time.sleep(1)\n",
    "done = True\n",
    "\n",
    "# fs= 16000\n",
    "# # second =  int(input(\"Enter time duration in seconds: \"))\n",
    "# second = 8\n",
    "# print(\"Please speak in Nepali\")\n",
    "# print(\"Recording.....\\n\")\n",
    "# record_voice = sounddevice.rec( int ( second * fs ) , samplerate = fs , channels = 1, dtype='int16' )\n",
    "# sounddevice.wait()\n",
    "aud_path=r\"D:\\New folder\\TOBECOPIEDTONEWLAPTOP\\THESIS Important\\THESIS CODING\\ActivelearningLabsharing\\bestMODELS\\Audio_liveRecords/Audiotest.wav\"\n",
    "write(aud_path,fs,record_voice)\n",
    "print(\"Saved in REC folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87d65ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing the recorded audio\n"
     ]
    }
   ],
   "source": [
    "# #Playing the recorded audio\n",
    "# # import required module\n",
    "# from playsound import playsound\n",
    "# # for playing note.wav file\n",
    "# print(\"Playing the recorded audio\")\n",
    "# playsound(aud_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37554a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the transcription\n",
      "prediction:  <सरलाई बोल्दै बोल्दैन>\n"
     ]
    }
   ],
   "source": [
    "#Prediction of the recorded audio.\n",
    "print(\"Loading the transcription\")\n",
    "path=r\"D:\\New folder\\TOBECOPIEDTONEWLAPTOP\\THESIS Important\\THESIS CODING\\ActivelearningLabsharing\\bestMODELS\\Audio_liveRecords/Audiotest.wav\"\n",
    "\n",
    "# print(path)\n",
    "\n",
    "x = path_to_audio(path)\n",
    "#print(x)\n",
    "x = tf.expand_dims(x, axis=0)\n",
    "# print(x.shape)\n",
    "idx_to_char = vectorizer.get_vocabulary()\n",
    "preds = model.generate(x, 2)\n",
    "preds = preds.numpy()\n",
    "bs = tf.shape(x)[0]\n",
    "for i in range(bs):\n",
    "    prediction = \"\"\n",
    "    for idx in preds[i, :]:\n",
    "        prediction += idx_to_char[idx]\n",
    "        if idx == 3:\n",
    "            break\n",
    "            \n",
    "print(\"prediction: \", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d49ad60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1c7585c0370>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = r\"F:\\\\TRAINEDModelSfromActiveLearningLabSharing\\\\Untitled Folder\\\\ModelFor99isTo1\\\\Checkpoints for 99 is to 1\\\\smallDatasetCheckpoint-56-0.12\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db483aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitl here for demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d58bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60d0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80dd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a36c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69fdab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84a65f67",
   "metadata": {},
   "source": [
    "# New WER calcualtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c95bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link for this: https://pyzone.dev/word-error-rate-in-python/\n",
    "\n",
    "weer_list=[]\n",
    "def wer(ref, hyp ,debug=True):\n",
    "    r = ref.split()\n",
    "    h = hyp.split()\n",
    "    #costs will holds the costs, like in the Levenshtein distance algorithm\n",
    "    costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    "    # backtrace will hold the operations we've done.\n",
    "    # so we could later backtrace, like the WER algorithm requires us to.\n",
    "    backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    " \n",
    "    OP_OK = 0\n",
    "    OP_SUB = 1\n",
    "    OP_INS = 2\n",
    "    OP_DEL = 3\n",
    "    DEL_PENALTY = 1\n",
    "    INS_PENALTY = 1\n",
    "    SUB_PENALTY = 1\n",
    "    \n",
    "    # First column represents the case where we achieve zero\n",
    "    # hypothesis words by deleting all reference words.\n",
    "    for i in range(1, len(r)+1):\n",
    "        costs[i][0] = DEL_PENALTY*i\n",
    "        backtrace[i][0] = OP_DEL\n",
    "    \n",
    "    # First row represents the case where we achieve the hypothesis\n",
    "    # by inserting all hypothesis words into a zero-length reference.\n",
    "    for j in range(1, len(h) + 1):\n",
    "        costs[0][j] = INS_PENALTY * j\n",
    "        backtrace[0][j] = OP_INS\n",
    "    \n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                costs[i][j] = costs[i-1][j-1]\n",
    "                backtrace[i][j] = OP_OK\n",
    "            else:\n",
    "                substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1\n",
    "                insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1\n",
    "                deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1\n",
    "                 \n",
    "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
    "                if costs[i][j] == substitutionCost:\n",
    "                    backtrace[i][j] = OP_SUB\n",
    "                elif costs[i][j] == insertionCost:\n",
    "                    backtrace[i][j] = OP_INS\n",
    "                else:\n",
    "                    backtrace[i][j] = OP_DEL\n",
    "                 \n",
    "    # back trace though the best route:\n",
    "    i = len(r)\n",
    "    j = len(h)\n",
    "    numSub = 0\n",
    "    numDel = 0\n",
    "    numIns = 0\n",
    "    numCor = 0\n",
    "    if debug:\n",
    "        print(\"OP\\tREF\\tHYP\")\n",
    "        lines = []\n",
    "    while i > 0 or j > 0:\n",
    "        if backtrace[i][j] == OP_OK:\n",
    "            numCor += 1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"OK\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_SUB:\n",
    "            numSub +=1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_INS:\n",
    "            numIns += 1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
    "        elif backtrace[i][j] == OP_DEL:\n",
    "            numDel += 1\n",
    "            i-=1\n",
    "            if debug:\n",
    "                lines.append(\"DEL\\t\" + r[i]+\"\\t\"+\"****\")\n",
    "    if debug:\n",
    "        lines = reversed(lines)\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "        print(\"#cor \" + str(numCor))\n",
    "        print(\"#sub \" + str(numSub))\n",
    "        print(\"#del \" + str(numDel))\n",
    "        print(\"#ins \" + str(numIns))\n",
    "    # return (numSub + numDel + numIns) / (float) (len(r))\n",
    "    wer_result = round( (numSub + numDel + numIns) / (float) (len(r)), 3)\n",
    "    print(\"wer:\",wer_result)\n",
    "    weer_list.append(wer_result)\n",
    "    return {'WER':wer_result, 'numCor':numCor, 'numSub':numSub, 'numIns':numIns, 'numDel':numDel, \"numCount\": len(r)}\n",
    "# Avg_wer=sum(weer_list) / len(weer_list)\n",
    "# print(Avg_wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec5d88",
   "metadata": {},
   "source": [
    "## 1. For 2064 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4787bff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <रुसको सङ्घीय विषय रुसको क्राइ>\n",
      "Prediction: <रुसको सङ्ख्य विषय रोसको क्राइ>\n",
      "Sen_no: 150\n",
      "OP\tREF\tHYP\n",
      "OK\t<रुसको\t<रुसको\n",
      "SUB\tसङ्घीय\tसङ्ख्य\n",
      "OK\tविषय\tविषय\n",
      "SUB\tरुसको\tरोसको\n",
      "OK\tक्राइ>\tक्राइ>\n",
      "#cor 3\n",
      "#sub 2\n",
      "#del 0\n",
      "#ins 0\n",
      "wer: 0.4\n",
      "[0.4]\n",
      "Reference: <बाइनरी ताराहरूको वर्ग>\n",
      "Prediction: <बाइनरी ताराहरूको वर्ग>\n",
      "Sen_no: 151\n",
      "OP\tREF\tHYP\n",
      "OK\t<बाइनरी\t<बाइनरी\n",
      "OK\tताराहरूको\tताराहरूको\n",
      "OK\tवर्ग>\tवर्ग>\n",
      "#cor 3\n",
      "#sub 0\n",
      "#del 0\n",
      "#ins 0\n",
      "wer: 0.0\n",
      "[0.4, 0.0]\n",
      "Reference: <उन्नाइस सय छयानब्बे र सन्तानब्बेमा कोलम्बियाली फुटबल व्यावसायिकको पहिलो श्रेणी>\n",
      "Prediction: <उन्नाइस सय छाया नबेर सन्तानबेमा कोलम्बेर साएको पहिलो स्वर्ग व्यवसायिको पहिलो स्रेनी>\n",
      "Sen_no: 152\n",
      "OP\tREF\tHYP\n",
      "OK\t<उन्नाइस\t<उन्नाइस\n",
      "OK\tसय\tसय\n",
      "INS\t****\tछाया\n",
      "INS\t****\tनबेर\n",
      "SUB\tछयानब्बे\tसन्तानबेमा\n",
      "SUB\tर\tकोलम्बेर\n",
      "SUB\tसन्तानब्बेमा\tसाएको\n",
      "SUB\tकोलम्बियाली\tपहिलो\n",
      "SUB\tफुटबल\tस्वर्ग\n",
      "SUB\tव्यावसायिकको\tव्यवसायिको\n",
      "OK\tपहिलो\tपहिलो\n",
      "SUB\tश्रेणी>\tस्रेनी>\n",
      "#cor 3\n",
      "#sub 7\n",
      "#del 0\n",
      "#ins 2\n",
      "wer: 0.9\n",
      "[0.4, 0.0, 0.9]\n",
      "[0.4, 0.0, 0.9]\n",
      "the avg cer is:\n",
      "0.43333333333333335\n"
     ]
    }
   ],
   "source": [
    "#TESTING FROM ANOTHER DATASETS ( NEEDS MORE TRAINING EPOCHS BEFORE GOOD DEMO)\n",
    "myfile = open(r'D:\\shishir_ml\\Untitled Folder\\my_Test_datasetss\\line_index.tsv', 'r',encoding = 'utf-8')\n",
    "Lines = myfile.readlines()\n",
    "from pydub import AudioSegment as am\n",
    "\n",
    "count = 0\n",
    "my_lisst=[]\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    count += 1\n",
    "    # wordarr = line.split()\n",
    "    arr = line.split(\"\\t\")\n",
    "    my_lisst.append(arr)\n",
    "    # print (arr)       \n",
    "# audio_iddd=arr[0]\n",
    "# audio_text=arr[2]\n",
    "    # import required module\n",
    "\n",
    "from playsound import playsound\n",
    "\n",
    "# path=\"D:\\\\shishir_ml\\\\Untitled Folder\\\\tess_1.wav\"\n",
    "\n",
    "weer_list=[]\n",
    "# for xxz in range(2063):\n",
    "for xxz in range(150,153):\n",
    "\n",
    "    filepath= r\"D:\\shishir_ml\\Untitled Folder\\my_Test_datasetss\\wavs/\" + my_lisst[xxz][0]+\".wav\" #the audio\n",
    "#     print(\"Playing the audio file...\")\n",
    "#     playsound(filepath)\n",
    "    saving_dir=f\"D:\\\\shishir_ml\\\\Untitled Folder\\\\my_Test_datasets\\\\Converted\\\\{xxz}.wav\"\n",
    "    sound = am.from_file(filepath, format='wav')\n",
    "    sound = sound.set_frame_rate(16000)\n",
    "    sound.export(saving_dir, format='wav')\n",
    "    pathh= saving_dir\n",
    "    # for playing note.wav file\n",
    " \n",
    "\n",
    "    referencetxttz=my_lisst[xxz][1]\n",
    "    x = path_to_audio(pathh)\n",
    "    #print(x)\n",
    "    x = tf.expand_dims(x, axis=0)\n",
    "    # print(x.shape)\n",
    "    idx_to_char = vectorizer.get_vocabulary()\n",
    "    preds = model.generate(x, 2)\n",
    "    preds = preds.numpy()\n",
    "    bs = tf.shape(x)[0]\n",
    "    for i in range(bs):\n",
    "        prediction = \"\"\n",
    "        for idx in preds[i, :]:\n",
    "            prediction += idx_to_char[idx]\n",
    "            if idx == 3:\n",
    "                break\n",
    "#     print(\"reference:\", referencetxtt)\n",
    "#     print(\"prediction: \", prediction,\"\\n\")\n",
    "    \n",
    "\n",
    "    new=referencetxttz.replace(\"\\n\", \"\")\n",
    "    new_ref=\"<\"+new+\">\"\n",
    "    print(\"Reference:\",new_ref)\n",
    "    print(\"Prediction:\",prediction)\n",
    "    \n",
    "#     print(new_ref.split(),hyp.split())\n",
    "#     totError=wer(new_ref,hyp)\n",
    "#     totworderror=wer(new_ref.split(),hyp.split())\n",
    "#     print(\"totalCharError:\",totError)\n",
    "#     print(\"totalWordError:\",totworderror)\n",
    "#     weerr=(totError/len(new_ref))*100\n",
    "#     wwerrr=(totworderror/len(new_ref.split()))*100\n",
    "#     weerr=wer(new_ref,hyp) \n",
    "\n",
    "    print(\"Sen_no:\",xxz)\n",
    "    ref=new_ref\n",
    "    hyp=prediction\n",
    "    wer(ref,hyp)\n",
    "    print(weer_list)\n",
    "    \n",
    "    \n",
    "print(weer_list)\n",
    "print(\"the avg wer is:\")\n",
    "Avg_wer=sum(weer_list) / len(weer_list)\n",
    "print(Avg_wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6ee9ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of WER values: 3\n",
      "0.43333333333333335\n"
     ]
    }
   ],
   "source": [
    "print(\"total no of WER values:\",len(weer_list))\n",
    "Avg_WER=sum(weer_list)/len(weer_list)\n",
    "print(Avg_WER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b2d18b",
   "metadata": {},
   "source": [
    "## 2. For 166K dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4e12263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: <हो भने नदी>\n",
      "Prediction: <हो भनिन् ती>\n",
      "Sen_no: 157725\n",
      "OP\tREF\tHYP\n",
      "OK\t<हो\t<हो\n",
      "SUB\tभने\tभनिन्\n",
      "SUB\tनदी>\tती>\n",
      "#cor 1\n",
      "#sub 2\n",
      "#del 0\n",
      "#ins 0\n",
      "wer: 0.667\n",
      "[0.667]\n",
      "Reference: <हो भने नदी>\n",
      "Prediction: <हो भने नदी>\n",
      "Sen_no: 157726\n",
      "OP\tREF\tHYP\n",
      "OK\t<हो\t<हो\n",
      "OK\tभने\tभने\n",
      "OK\tनदी>\tनदी>\n",
      "#cor 3\n",
      "#sub 0\n",
      "#del 0\n",
      "#ins 0\n",
      "wer: 0.0\n",
      "[0.667, 0.0]\n",
      "Reference: <हो भने निर्माणस्थलमा>\n",
      "Prediction: <हो भने निर्माण स्थलमा>\n",
      "Sen_no: 157727\n",
      "OP\tREF\tHYP\n",
      "OK\t<हो\t<हो\n",
      "OK\tभने\tभने\n",
      "INS\t****\tनिर्माण\n",
      "SUB\tनिर्माणस्थलमा>\tस्थलमा>\n",
      "#cor 2\n",
      "#sub 1\n",
      "#del 0\n",
      "#ins 1\n",
      "wer: 0.667\n",
      "[0.667, 0.0, 0.667]\n",
      "Reference: <हो भने निर्माणस्थलमा>\n",
      "Prediction: <भूभने निर्माण स्तरमा>\n",
      "Sen_no: 157728\n",
      "OP\tREF\tHYP\n",
      "SUB\t<हो\t<भूभने\n",
      "SUB\tभने\tनिर्माण\n",
      "SUB\tनिर्माणस्थलमा>\tस्तरमा>\n",
      "#cor 0\n",
      "#sub 3\n",
      "#del 0\n",
      "#ins 0\n",
      "wer: 1.0\n",
      "[0.667, 0.0, 0.667, 1.0]\n",
      "Reference: <हो भने निर्माणस्थलमा>\n",
      "Prediction: <कोबने निर्माणिस्तान>\n",
      "Sen_no: 157729\n",
      "OP\tREF\tHYP\n",
      "DEL\t<हो\t****\n",
      "SUB\tभने\t<कोबने\n",
      "SUB\tनिर्माणस्थलमा>\tनिर्माणिस्तान>\n",
      "#cor 0\n",
      "#sub 2\n",
      "#del 1\n",
      "#ins 0\n",
      "wer: 1.0\n",
      "[0.667, 0.0, 0.667, 1.0, 1.0]\n",
      "[0.667, 0.0, 0.667, 1.0, 1.0]\n",
      "the avg wer is:\n",
      "0.6668000000000001\n"
     ]
    }
   ],
   "source": [
    "#PREDICTING FROM TEST DATA Alredy downsampled\n",
    "# PREDICTION and WER EVALUATION FOR ANYDATA.\n",
    "myfile = open('D:\\\\shishir_ml\\\\Untitled Folder\\datasets\\LJSpeech\\\\utt_spk_text.tsv', 'r',encoding = 'utf-8')\n",
    "Lines = myfile.readlines()\n",
    "\n",
    "count = 0\n",
    "my_list=[]\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    count += 1\n",
    "    # wordarr = line.split()\n",
    "    arr = line.split(\"\\t\")\n",
    "    my_list.append(arr)\n",
    "    # print (arr)       \n",
    "# audio_iddd=arr[0]\n",
    "# audio_text=arr[2]\n",
    "    # import required module\n",
    "from playsound import playsound\n",
    "\n",
    "# path=\"D:\\\\shishir_ml\\\\Untitled Folder\\\\tess_1.wav\"\n",
    "\n",
    "weer_list=[]\n",
    "for xz in range(157725,157730):\n",
    "# for xz in range(1):\n",
    "\n",
    "    path=\"D:\\\\shishir_ml\\\\Untitled Folder\\datasets\\LJSpeech/wavs/\" + my_list[xz][0]+\".wav\" #the audio\n",
    "\n",
    "\n",
    "    # for playing note.wav file\n",
    "#     print(\"Playing the audio file\")\n",
    "#     playsound(path)\n",
    "    \n",
    "    referencetxtt=my_list[xz][2]\n",
    "    x = path_to_audio(path)\n",
    "    #print(x)\n",
    "    x = tf.expand_dims(x, axis=0)\n",
    "    # print(x.shape)\n",
    "    idx_to_char = vectorizer.get_vocabulary()\n",
    "    preds = model.generate(x, 2)\n",
    "    preds = preds.numpy()\n",
    "    bs = tf.shape(x)[0]\n",
    "    for i in range(bs):\n",
    "        prediction = \"\"\n",
    "        for idx in preds[i, :]:\n",
    "            prediction += idx_to_char[idx]\n",
    "            if idx == 3:\n",
    "                break\n",
    "#     print(\"reference:\", referencetxtt)\n",
    "#     print(\"prediction: \", prediction,\"\\n\") prediction,\"\\n\")\n",
    "    \n",
    "\n",
    "    new=referencetxtt.replace(\"\\n\", \"\")\n",
    "    new_ref=\"<\"+new+\">\"\n",
    "    print(\"Reference:\",new_ref)\n",
    "    print(\"Prediction:\",prediction)\n",
    "    \n",
    "#     print(new_ref.split(),hyp.split())\n",
    "#     totError=wer(new_ref,hyp)\n",
    "#     totworderror=wer(new_ref.split(),hyp.split())\n",
    "#     print(\"totalCharError:\",totError)\n",
    "#     print(\"totalWordError:\",totworderror)\n",
    "#     weerr=(totError/len(new_ref))*100\n",
    "#     wwerrr=(totworderror/len(new_ref.split()))*100\n",
    "#     weerr=wer(new_ref,hyp) \n",
    "\n",
    "    print(\"Sen_no:\",xz)\n",
    "    ref=new_ref\n",
    "    hyp=prediction\n",
    "    wer(ref,hyp)\n",
    "    print(weer_list)\n",
    "    \n",
    "    \n",
    "print(weer_list)\n",
    "print(\"the avg wer is:\")\n",
    "Avg_wer=sum(weer_list) / len(weer_list)\n",
    "print(Avg_wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40f18ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of WER values: 5\n",
      "0.5334\n"
     ]
    }
   ],
   "source": [
    "print(\"total no of WER values:\",len(weer_list))\n",
    "Avg_WER=sum(weer_list)/len(weer_list)\n",
    "print(Avg_WER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60089cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_np_female=[0.4, 0.0, 1.1, 0.571, 0.111, 0.571, 0.167, 0.583, 0.333, 0.545, 0.769, 0.6, 1.0, 0.667, 0.333, 0.0, 0.5, 0.556, 0.333, 0.769, 0.615, 0.2, 0.333, 0.556, 0.75, 0.111, 0.4, 0.429, 0.0, 0.5, 0.778, 1.182, 0.688, 0.75, 0.778, 0.75, 0.556, 1.0, 0.5, 0.778, 1.083, 0.429, 0.6, 0.455, 0.909, 0.333, 1.0, 0.5, 0.5, 0.286, 0.286, 0.636, 0.091, 1.0, 0.588, 0.9, 0.6, 0.882, 0.667, 0.667, 0.2, 0.444, 0.6, 1.0, 0.727, 0.85, 0.667, 0.5, 0.25, 1.0, 0.667, 0.455, 0.25, 0.6, 0.143, 0.375, 0.636, 0.444, 0.222, 0.444, 0.944, 0.75, 0.667, 0.545, 0.8, 0.5, 0.667, 0.778, 0.75, 0.667, 0.778, 0.444, 0.786, 0.4, 0.2, 0.714, 0.5, 0.583, 0.643, 0.846, 0.167, 0.667, 0.222, 0.556, 0.7, 0.7, 0.8, 0.933, 0.667, 0.875, 0.75, 0.5, 0.75, 0.556, 1.5, 0.7, 0.333, 0.375, 0.6, 0.556, 0.8, 0.333, 0.75, 0.6, 0.333, 0.0, 0.333, 0.4, 0.267, 0.0, 0.895, 1.6, 0.5, 0.333, 0.4, 0.75, 1.364, 1.0, 0.538, 0.0, 0.2, 0.4, 0.769, 0.5, 0.429, 0.667, 0.2, 0.25, 0.444, 0.6, 0.778, 0.0, 0.429, 1.2, 0.25, 0.0, 0.2, 0.111, 0.083, 0.444, 0.267, 0.545, 0.3, 0.333, 0.444, 0.5, 0.333, 0.333, 0.9, 0.5, 0.375, 0.0, 0.842, 0.692, 0.444, 0.75, 0.571, 1.0, 0.429, 0.667, 0.333, 1.0, 0.222, 0.429, 1.0, 0.5, 0.4, 1.0, 0.462, 0.0, 0.385, 0.167, 0.688, 0.0, 0.385, 0.0, 0.375, 0.167, 0.625, 0.75, 0.667, 0.6, 0.444, 0.333, 0.333, 0.6, 0.4, 0.2, 0.4, 0.333]\n",
    "\n",
    "merged_all=[0.333, 0.0, 1.0, 0.0, 0.667, 0.5, 1.5, 1.5, 0.0, 1.0, 1.0, 0.667, 0.667, 0.0, 1.0, 0.667, 0.667, 0.667, 0.0, 0.0, 0.333, 1.0, 0.667, 0.5, 0.75, 0.0, 0.667, 0.667, 0.0, 1.0, 1.0, 0.667, 0.0, 0.0, 0.667, 0.333, 0.333, 1.0, 0.0, 0.0, 0.333, 0.75, 0.333, 0.0, 0.75, 0.5, 1.273, 0.333, 0.0, 0.5, 1.0, 1.0, 0.333, 0.0, 0.5, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.667, 0.667, 0.0, 1.0, 0.0, 0.333, 0.333, 0.333, 0.667, 0.667, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.667, 0.0, 1.0, 1.0, 0.667, 0.667, 0.667, 0.0, 0.333, 0.333, 0.333, 0.667, 0.0, 0.333, 0.667, 0.333, 0.0, 0.667, 0.5, 1.5, 0.5, 1.0, 1.625, 1.667, 0.0, 2.0, 1.0, 0.5, 2.0, 0.5, 0.333, 0.0, 0.667, 0.0, 0.667, 0.0, 0.333, 0.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.5, 0.667, 0.333, 0.667, 0.5, 0.5, 0.5, 1.5, 1.0, 0.667, 1.333, 0.333, 1.0, 0.667, 1.5, 1.0, 0.667, 0.333, 1.0, 0.333, 0.333, 1.0, 0.333, 0.333, 0.667, 0.0, 0.5, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 1.333, 1.0, 0.667, 0.333, 0.333, 0.5, 0.667, 1.0, 1.0, 0.0, 0.667, 0.4, 0.0, 0.0, 0.333, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 1.0, 0.667, 0.333, 0.667, 0.0, 0.0, 0.333, 0.0, 0.0, 0.333, 0.333, 0.667, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.5, 0.0, 0.5, 0.0, 0.0, 0.667, 1.0, 0.0, 0.5, 0.5, 0.8, 0.4, 0.667, 0.0, 0.0, 0.0, 0.667, 0.333, 1.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.667, 1.0, 1.0, 1.0, 0.5, 0.667, 1.0, 0.333, 0.5, 1.5, 1.0, 1.0, 0.0, 0.0, 0.333, 0.333, 0.5, 1.0, 0.0, 1.0, 0.0, 0.333, 0.0, 1.0, 0.0, 0.0, 0.667, 0.667, 0.333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.333, 0.0, 0.667, 0.333, 1.0, 0.0, 0.333, 0.667, 0.667, 0.333, 1.0, 0.667, 0.667, 0.667, 0.5, 1.5, 1.5, 0.5, 0.5, 1.0, 1.5, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.333, 1.0, 0.333, 0.5, 0.5, 0.333, 0.625, 0.875, 0.8, 0.667, 0.667, 0.667, 1.0, 0.25, 0.75, 1.0, 2.0, 2.333, 1.0, 1.0, 0.667, 0.667, 0.333, 0.0, 1.0, 0.667, 0.667, 0.0, 0.727, 0.667, 0.667, 0.667, 1.0, 0.333, 0.333, 0.667, 1.0, 1.0, 1.0, 1.333, 2.667, 0.0, 1.5, 2.0, 0.5, 0.5, 0.0, 0.333, 0.667, 1.0, 0.667, 0.667, 0.5, 0.0, 0.5, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.333, 0.0, 0.333, 2.0, 2.0, 0.333, 0.667, 0.333, 0.0, 0.667, 1.0, 0.0, 0.333, 0.333, 1.0, 0.333, 0.667, 0.0, 1.0, 0.333, 0.333, 0.667, 1.667, 0.0, 0.0, 0.0, 0.333, 1.0, 0.667, 0.667, 0.333, 0.667, 1.0, 1.0, 0.333, 1.0, 0.667, 1.0, 0.0, 0.667, 0.625, 1.0, 0.667, 0.0, 0.667, 0.667, 0.667, 0.222, 0.444, 1.0, 0.5, 0.333, 0.0, 0.0, 2.0, 1.0, 0.667, 1.0, 1.333, 1.333, 1.0, 0.667, 0.0, 2.0, 0.667, 1.0, 0.0, 0.0, 0.0, 0.667, 0.333, 1.0, 0.667, 1.0, 0.333, 0.667, 0.333, 0.333, 0.333, 0.333, 0.0, 1.0, 0.0, 1.0, 1.667, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.333, 1.333, 1.333, 0.0, 0.667, 0.667, 0.667, 0.667, 0.0, 0.333, 0.0, 0.667, 0.333, 0.667, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 0.333, 0.333, 1.0, 1.0, 0.333, 0.667, 0.667, 0.846, 1.0, 0.0, 0.333, 0.333, 0.0, 0.333, 0.333, 0.0, 0.5, 0.5, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.333, 1.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.333, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.667, 0.333, 0.333, 0.333, 0.333, 0.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.333, 0.333, 0.4, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.5, 0.0, 0.0, 0.333, 1.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.5, 0.333, 0.0, 0.333, 0.333, 0.333, 0.667, 0.667, 0.0, 0.667, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.333, 0.0, 1.0, 1.0, 0.0, 0.333, 0.667, 0.667, 0.333, 0.0, 0.0, 0.667, 0.0, 0.333, 0.667, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 1.0, 0.0, 0.0, 0.333, 0.667, 0.667, 0.0, 0.667, 0.667, 0.333, 1.0, 0.333, 0.667, 0.5, 0.0, 0.333, 0.333, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.5, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.667, 0.667, 0.0, 0.0, 0.333, 0.0, 0.0, 1.0, 1.0, 0.667, 1.0, 1.0, 0.333, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 1.0, 0.75, 0.75, 0.0, 0.5, 0.5, 1.25, 1.0, 0.333, 0.667, 0.667, 1.0, 0.875, 0.0, 0.333, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.667, 0.333, 0.0, 0.5, 0.556, 0.0, 0.667, 0.667, 0.0, 1.0, 0.0, 0.0, 0.333, 1.0, 1.0, 0.333, 0.333, 1.0, 0.5, 0.667, 0.667, 1.0, 0.5, 0.5, 0.667, 0.667, 0.0, 0.0, 0.0, 1.0, 0.0, 0.333, 0.0, 0.667, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.667, 1.0, 0.0, 0.0, 0.667, 0.333, 0.0, 0.2, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.75, 0.25, 0.5, 1.0, 0.0, 0.333, 0.667, 0.0, 0.333, 0.0, 0.0, 0.0, 1.0, 0.667, 0.0, 0.333, 0.333, 0.667, 0.778, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.0, 0.333, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.667, 0.5, 0.75, 0.25, 0.667, 0.667, 0.667, 0.667, 0.667, 0.333, 0.333, 0.333, 0.667, 0.0, 1.0, 0.667, 0.333, 0.667, 0.0, 0.667, 0.333, 0.0, 0.333, 0.333, 0.333, 0.667, 0.0, 1.0, 0.0, 0.0, 0.667, 0.667, 0.667, 1.0, 0.25, 0.0, 0.333, 0.667, 0.0, 0.5, 0.25, 0.0, 0.333, 1.0, 0.0, 0.333, 0.0, 0.667, 0.667, 0.0, 0.0, 0.0, 0.333, 0.25, 0.333, 1.0, 0.0, 0.333, 0.0, 1.667, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.667, 0.667, 0.667, 0.0, 0.0, 0.0, 0.0, 0.25, 0.333, 0.667, 0.667, 0.0, 1.0, 1.0, 0.0, 0.0, 0.333, 0.333, 0.667, 1.0, 0.667, 0.667, 0.0, 0.0, 0.333, 1.0, 0.0, 0.333, 1.0, 0.0, 0.667, 0.0, 0.667, 0.333, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 1.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.5, 1.0, 0.667, 1.0, 0.0, 0.25, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 1.0, 0.0, 0.0, 0.667, 0.5, 0.0, 0.0, 0.0, 0.5, 0.667, 1.0, 0.667, 1.0, 0.667, 0.333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.333, 0.667, 1.0, 0.667, 0.25, 0.5, 0.0, 0.0, 0.667, 0.667, 0.0, 0.0, 0.25, 0.0, 0.5, 0.333, 0.0, 0.333, 0.333, 0.333, 0.667, 0.333, 0.5, 1.0, 0.667, 0.667, 1.333, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.333, 0.333, 0.667, 0.333, 0.333, 0.0, 0.667, 0.6, 0.667, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 1.333, 0.0, 0.333, 0.333, 0.0, 0.333, 0.0, 0.0, 1.0, 0.0, 0.667, 0.333, 0.0, 0.667, 0.333, 1.0, 0.0, 0.25, 0.667, 0.0, 1.0, 0.0, 0.0, 0.333, 1.0, 1.0, 0.333, 1.0, 1.0, 0.333, 0.333, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.667, 0.5, 0.0, 0.5, 0.25, 1.0, 0.0, 1.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 1.0, 0.0, 4.0, 0.333, 1.0, 1.333, 0.667, 1.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.5, 0.5, 0.5, 1.0, 0.0, 0.333, 0.333, 0.333, 0.0, 0.0, 1.0, 0.5, 0.0, 0.333, 1.0, 0.667, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.5, 0.0, 0.0, 0.333, 0.0, 1.0, 0.0, 0.667, 0.667, 1.0, 1.0, 0.667, 0.0, 0.75, 0.0, 0.75, 0.667, 0.333, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 0.0, 0.333, 0.333, 0.75, 0.25, 0.667, 0.667, 0.333, 0.0, 1.0, 0.333, 0.0, 0.333, 0.667, 0.667, 0.0, 0.0, 0.333, 0.333, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 1.0, 1.0, 1.0, 0.6, 0.0, 0.0, 0.667, 0.667, 0.667, 0.0, 0.0, 0.375, 1.0, 0.0, 0.0, 1.0, 0.667, 0.667, 0.6, 0.6, 0.333, 0.5, 0.667, 0.667, 0.333, 0.333, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.333, 0.333, 1.0, 0.667, 0.667, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 1.0, 0.667, 0.333, 0.333, 0.333, 0.75, 0.0, 0.0, 1.333, 0.0, 0.667, 0.333, 0.333, 0.5, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.333, 0.0, 0.0, 0.333, 0.0, 0.333, 0.333, 0.333, 0.333, 0.667, 0.0, 0.0, 0.667, 0.333, 0.5, 0.0, 0.25, 0.5, 0.0, 0.333, 0.0, 0.0, 0.0, 0.667, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 0.333, 0.0, 0.25, 0.0, 0.25, 0.0, 0.333, 0.5, 0.0, 0.0, 1.333, 1.0, 1.0, 0.667, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.25, 0.333, 0.333, 1.5, 1.5, 1.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 1.0, 1.0, 0.333, 0.5, 0.0, 0.333, 0.333, 0.818, 0.364, 0.182, 1.0, 0.667, 0.667, 0.333, 0.667, 0.667, 1.0, 1.0, 0.667, 0.0, 0.667, 0.333, 0.0, 0.0, 0.0, 1.333, 1.333, 0.5, 0.667, 1.0, 1.333, 0.0, 0.0, 0.2, 0.2, 0.333, 0.0, 1.0, 0.0, 0.667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.25, 0.5, 0.0, 0.25, 0.0, 0.0, 1.0, 0.667, 0.0, 0.333, 0.333, 0.0, 0.667, 1.0, 0.333, 0.0, 0.0, 1.333, 0.75, 1.0, 0.0, 0.333, 0.333, 0.0, 0.5, 0.667, 0.0, 0.0, 0.667, 0.333, 0.667, 0.0, 0.333, 0.333, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.667, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.667, 0.667, 0.0, 0.0, 1.5, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.333, 0.75, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.333, 0.333, 0.333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.333, 0.667, 1.0, 1.667, 1.0, 1.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.667, 0.0, 0.0, 1.0, 1.0, 0.333, 0.0, 1.0, 0.0, 0.333, 0.333, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 1.0, 0.667, 0.667, 0.333, 0.0, 1.0, 0.333, 0.667, 0.667, 0.333, 0.0, 0.0, 1.5, 0.0, 0.333, 0.333, 0.5, 0.0, 0.667, 1.0, 1.0, 0.667, 1.0, 1.333, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 1.333, 0.667, 0.333, 0.0, 0.0, 0.667, 0.0, 0.0, 0.333, 1.0, 0.333, 0.0, 1.0, 0.333, 0.0, 0.0, 1.0, 1.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.667, 0.667, 0.333, 0.0, 0.667, 0.0, 0.667, 1.0, 0.333, 0.333, 0.0, 0.667, 0.667, 0.333, 1.333, 0.667, 0.667, 1.0, 0.0, 0.333, 0.333, 0.0, 0.333, 1.0, 0.0, 1.0, 0.0, 0.667, 0.0, 0.333, 0.0, 0.0, 0.667, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.5, 0.5, 0.333, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 0.667, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.333, 0.667, 0.0, 0.333, 0.333, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.5, 0.0, 0.667, 0.0, 0.5, 0.0, 0.5, 0.75, 0.25, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.667, 0.333, 0.333, 0.0, 0.667, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.333, 0.333, 0.667, 0.333, 0.667, 0.0, 0.667, 1.0, 0.667, 0.0, 0.667, 1.0, 1.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.667, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.333, 0.0, 0.5, 0.0, 0.0, 0.333, 0.667, 0.0, 0.333, 0.0, 0.667, 0.0, 0.0, 0.5, 0.667, 0.0, 0.0, 0.5, 0.5, 0.0, 0.333, 0.0, 0.5, 0.0, 1.0, 0.333, 1.0, 0.333, 0.667, 0.333, 0.667, 0.333, 0.333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.667, 1.0, 1.0, 0.0, 0.333, 0.333, 0.5, 0.333, 0.667, 1.0, 1.0, 0.5, 0.667, 0.667, 0.0, 0.667, 0.667, 3.667, 0.333, 1.0, 1.333, 1.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.333, 0.333, 0.667, 0.0, 0.0, 0.667, 0.0, 0.0, 0.333, 1.0, 0.667, 0.333, 0.0, 0.667, 0.0, 0.0, 0.667, 0.0, 2.0, 2.0, 0.333, 0.333, 0.5, 0.25, 0.75, 0.0, 1.0, 0.667, 0.667, 0.667, 0.667, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.333, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 1.0, 0.0, 0.5, 0.5, 0.5, 0.5, 0.0, 0.0, 1.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.75, 2.5, 0.75, 0.5, 0.0, 0.0, 0.333, 1.0, 0.0, 0.0, 0.333, 0.0, 2.333, 1.0, 0.333, 0.333, 0.333, 0.667, 0.0, 0.667, 0.667, 0.0, 0.667, 0.0, 0.0, 1.0, 0.333, 0.333, 0.0, 0.667, 1.0, 0.667, 0.333, 0.182, 0.636, 0.273, 1.333, 1.0, 0.667, 0.667, 1.333, 0.333, 0.667, 0.667, 0.667, 0.333, 0.0, 0.5, 0.0, 0.0, 0.333, 0.667, 0.333, 0.0, 0.667, 0.0, 0.0, 0.333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.333, 0.667, 0.667, 0.667, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.667, 0.667, 0.0, 0.0, 1.0, 0.333, 0.667, 1.0, 1.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 1.0, 0.333, 0.0, 0.333, 0.667, 0.333, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 0.5, 0.75, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2.0, 1.0, 1.333, 1.0, 0.667, 0.0, 1.0, 0.0, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.75, 0.0, 0.0, 0.0, 0.0, 0.333, 1.0, 0.333, 0.667, 0.333, 0.0, 0.0, 0.667, 0.667, 0.0, 0.667, 0.0, 0.0, 0.333, 0.333, 0.333, 0.333, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.667, 0.5, 0.5, 0.5, 1.667, 0.667, 0.667, 0.667, 0.0, 0.75, 1.0, 0.333, 0.667, 0.0, 0.667, 0.0, 0.0, 0.333, 0.0, 0.0, 0.5, 0.0, 0.667, 0.667, 0.0, 0.0, 0.333, 0.667, 0.333, 1.0, 0.333, 0.667, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.667, 0.333, 0.0, 0.333, 0.0, 0.5, 0.333, 0.333, 0.667, 1.0, 0.0, 1.0, 0.333, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.75, 1.0, 0.75, 0.667, 0.0, 0.0, 0.0, 0.0, 0.667, 1.0, 1.5, 0.5, 1.5, 0.333, 0.0, 0.333, 0.667, 0.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.667, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.667, 0.333, 0.0, 0.667, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.333, 0.667, 0.667, 0.5, 0.667, 0.667, 0.667, 0.0, 0.5, 0.5, 0.75, 0.25, 0.25, 1.0, 0.0, 0.0, 0.0, 0.667, 0.667, 0.667, 0.333, 1.0, 0.75, 0.667, 0.0, 1.0, 0.0, 0.5, 0.333, 2.0, 0.333, 0.667, 0.0, 0.0, 0.333, 0.333, 0.0, 0.333, 0.667, 0.733, 0.333, 0.333, 0.333, 0.667, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.333, 0.667, 0.667, 1.0, 0.667, 0.0, 0.333, 0.0, 0.667, 0.333, 0.667, 0.0, 0.333, 0.667, 0.0, 1.0, 0.333, 1.0, 0.667, 0.333, 0.667, 0.5, 0.5, 0.667, 0.333, 1.5, 0.0, 0.0, 0.667, 0.0, 0.667, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.667, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.333, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.333, 0.0, 0.333, 0.333, 0.25, 0.333, 0.667, 0.0, 0.25, 0.0, 0.667, 0.0, 0.333, 0.0, 0.333, 0.333, 1.0, 0.5, 0.667, 0.0, 1.0, 1.0, 1.5, 0.5, 1.0, 0.5, 0.5, 0.0, 2.0, 0.5, 0.333, 0.667, 0.0, 0.333, 0.333, 0.333, 0.333, 1.667, 0.333, 1.0, 1.0, 0.333, 0.5, 0.5, 0.667, 0.333, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.667, 0.5, 0.5, 0.0, 1.5, 0.5, 0.5, 0.5, 0.333, 0.333, 0.667, 1.5, 1.5, 1.0, 0.333, 0.667, 0.667, 1.0, 0.667, 0.333, 0.667, 0.333, 0.667, 0.667, 1.0, 0.667, 0.333, 0.667, 0.667, 0.667, 0.0, 0.0, 0.0, 0.667, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.5, 0.333, 0.0, 0.0, 0.333, 0.667, 0.333, 0.333, 0.5, 0.5, 0.0, 3.0, 0.333, 0.0, 0.75, 0.667, 1.0, 1.0, 2.0, 0.667, 0.333, 0.667, 1.0, 0.0, 1.0, 1.0, 1.0, 0.333, 0.0, 0.333, 0.333, 0.667, 0.0, 0.0, 0.0, 0.667, 1.0, 0.0, 0.333, 0.2, 0.667, 0.667, 0.667, 0.667, 0.0, 0.5, 0.0, 1.0, 1.333, 0.0, 0.667, 0.667, 0.667, 0.667, 0.667, 0.667, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.273, 0.364, 0.333, 0.333, 0.333, 0.333, 0.0, 0.667, 0.333, 0.0, 0.333, 0.0, 0.0, 0.5, 0.667, 0.0, 0.0, 0.0, 0.333, 0.667, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.333, 0.0, 0.333, 0.667, 0.667, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.667, 0.333, 1.0, 0.667, 0.333, 0.5, 0.5, 0.5, 0.333, 0.333, 0.333, 0.333, 0.0, 0.0, 0.0, 0.667, 0.0, 0.667, 0.0, 0.75, 1.0, 0.0, 0.333, 0.667, 0.0, 0.5, 0.0, 0.0, 0.5, 0.333, 0.0, 0.0, 0.0, 0.5, 0.5, 0.333, 0.0, 0.5, 1.0, 0.5, 0.5, 0.333, 0.333, 0.0, 0.333, 0.667, 0.667, 0.333, 0.333, 1.0, 0.333, 1.0, 1.333, 0.667, 0.667, 0.667, 1.0, 0.333, 0.5, 1.0, 0.667, 0.333, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.5, 0.5, 0.333, 0.667, 0.0, 0.333, 0.0, 0.5, 0.667, 0.667, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.667, 0.667, 1.0, 0.0, 0.0, 0.667, 0.667, 0.667, 0.667, 0.0, 0.0, 0.667, 0.667, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.5, 0.667, 0.333, 0.0, 0.0, 0.333, 0.333, 0.0, 0.667, 0.0, 0.0, 0.667, 1.0, 0.0, 0.667, 0.0, 0.0, 0.333, 0.5, 0.667, 0.333, 0.333, 0.333, 0.333, 0.333, 0.333, 0.667, 1.0, 0.667, 1.0, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.333, 0.333, 0.667, 0.667, 1.0, 0.333, 0.333, 0.667, 0.333, 0.5, 0.5, 0.5, 0.0, 0.667, 0.75, 0.5, 0.667, 0.333, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.667, 1.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.667, 0.667, 0.0, 0.333, 0.333, 0.333, 0.667, 0.667, 0.5, 0.0, 0.5, 0.333, 0.0, 1.0, 0.0, 1.0, 0.5, 0.333, 0.0, 0.667, 0.667, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.667, 0.333, 0.0, 0.333, 0.0, 0.667, 1.0, 0.333, 1.0, 0.333, 0.667, 0.556, 0.0, 1.0, 0.333, 0.667, 0.667, 0.333, 0.0, 0.667, 0.333, 0.0, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.5, 0.333, 0.0, 0.333, 0.333, 0.333, 0.333, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 1.25, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.667, 0.0, 0.0, 0.75, 0.583, 0.75, 0.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 1.0, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.333, 0.333, 0.0, 0.333, 1.0, 0.0, 1.0, 1.0, 0.333, 0.667, 0.333, 0.0, 0.667, 0.667, 0.333, 0.667, 0.0, 0.25, 0.5, 0.75, 1.0, 1.333, 0.333, 0.333, 0.333, 0.333, 1.333, 0.333, 0.0, 0.0, 0.0, 0.667, 1.0, 0.0, 0.0, 0.333, 0.333, 0.333, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.667, 0.333, 0.667, 0.5, 0.5, 0.0, 0.667, 0.333, 0.667, 0.0, 0.0, 1.0, 0.333, 0.333, 0.667, 0.333, 0.667, 0.0, 0.5, 0.5, 1.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.667, 0.333, 0.0, 0.333, 1.0, 1.0, 1.0, 0.0, 0.333, 0.333, 0.333, 0.333, 0.0, 0.0, 0.333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.667, 0.333, 0.0, 0.333, 0.0, 0.0, 0.333, 0.667, 0.333, 0.667, 0.333, 0.667, 0.5, 0.0, 0.667, 0.667, 0.0, 0.333, 0.0, 0.333, 0.333, 0.333, 0.333, 0.667, 0.333, 0.667, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 0.667, 0.333, 0.0, 0.0, 1.333, 0.667, 0.0, 0.667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.667, 0.333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.667, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.667, 0.0, 0.333, 0.667, 0.333, 0.0, 0.667, 1.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 1.0, 0.0, 0.0, 0.333, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.333, 0.786, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.333, 0.333, 0.5, 1.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.333, 0.0, 0.0, 1.0, 0.333, 0.333, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.667, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.667, 0.667, 0.333, 0.667, 0.0, 0.667, 1.0, 0.0, 0.5, 0.333, 0.0, 0.667, 0.333, 0.0, 0.0, 0.0, 1.333, 0.333, 0.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.5, 0.0, 0.667, 0.667, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.333, 1.0, 0.667, 1.0, 0.667, 0.667, 1.5, 0.5, 0.667, 0.667, 1.0, 1.0, 0.333, 0.6, 0.8, 0.8, 0.667, 0.5, 1.0, 0.0, 0.667, 0.5, 0.5, 0.0, 0.5, 0.75, 0.333, 1.333, 1.333, 0.0, 0.5, 0.75, 0.5, 0.5, 1.0, 0.5, 0.5, 1.0, 0.25, 0.333, 0.0, 0.0, 0.333, 0.7, 1.0, 0.25, 0.333, 1.0, 0.5, 1.0, 1.0, 1.0, 0.667, 1.0, 1.0, 0.75, 0.333, 1.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.667, 0.333, 0.0, 0.667, 0.0, 1.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.333, 0.667, 0.667, 0.333, 0.0, 0.0, 1.0, 0.333, 0.667, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.25, 0.5, 1.5, 0.5, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.333, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 1.0, 0.667, 0.667, 1.333, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.333, 0.667, 1.0, 0.667, 2.0, 1.0, 1.333, 0.0, 0.0, 0.333, 0.0, 0.5, 0.0, 0.333, 0.0, 1.0, 1.0, 0.5, 0.0, 0.769, 0.333, 0.667, 0.333, 0.75, 1.0, 0.333, 0.333, 0.333, 0.333, 0.667, 1.0, 1.0, 0.667, 0.333, 0.0, 0.667, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 1.333, 0.0, 1.5, 0.0, 0.0, 1.0, 0.0, 0.5, 0.667, 0.667, 0.364, 0.333, 0.333, 0.333, 0.667, 0.0, 0.0, 0.5, 1.0, 2.0, 0.0, 0.0, 0.0, 0.5, 0.75, 0.333, 0.0, 1.333, 1.5, 1.0, 1.0, 0.333, 0.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.333, 0.333, 0.667, 0.333, 1.0, 0.5, 0.333, 0.667, 0.667, 0.0, 0.667, 0.429, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.667, 0.667, 0.333, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.667, 1.333, 0.0, 1.0, 0.0, 1.0, 0.0, 0.667, 0.333, 0.5, 0.0, 0.0, 0.0, 0.333, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.333, 0.667, 0.5, 0.5, 0.667, 1.333, 0.5, 1.5, 0.667, 0.0, 1.0, 1.0, 0.333, 0.5, 0.333, 0.0, 0.667, 1.0, 0.667, 0.667, 0.0, 1.0, 0.667, 0.5, 0.333, 0.333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.75, 0.5, 0.667, 0.333, 0.333, 1.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.667, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.667, 0.5, 1.0, 0.5, 0.667, 0.0, 0.0, 1.0, 1.0, 0.667, 0.333, 1.0, 0.333, 0.0, 0.5, 0.333, 0.667, 0.333, 0.333, 0.667, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.667, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.333, 1.333, 0.667, 1.0, 0.667, 0.667, 0.667, 0.667, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.444, 0.0, 0.667, 0.667, 0.333, 0.667, 0.667, 1.0, 0.75, 0.0, 0.5, 1.0, 0.5, 0.5, 0.0, 0.0, 1.0, 0.333, 1.0, 0.333, 1.0, 0.0, 0.667, 0.0, 0.333, 0.333, 1.0, 0.0, 0.0, 0.333, 1.0, 0.667, 0.0, 0.333, 1.5, 0.5, 2.333, 0.333, 0.333, 0.667, 1.0, 0.5, 1.0, 0.667, 0.0, 0.5, 1.333, 0.667, 0.667, 1.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.75, 0.0, 0.75, 0.333, 0.0, 0.0, 0.25, 1.0, 0.0, 0.5, 0.333, 0.778, 0.556, 0.778, 0.667, 0.6, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 1.0, 0.333, 0.667, 0.667, 0.333, 0.0, 0.571, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.6, 0.2, 0.25, 0.0, 0.667, 0.0, 0.0, 0.667, 0.0, 0.667, 0.333, 0.667, 0.333, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.25, 0.0, 1.0, 1.333, 1.0, 0.667, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7, 0.9, 0.333, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.667, 0.667, 0.333, 0.667, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.667, 0.0, 0.667, 0.333, 1.0, 0.667, 0.667, 0.667, 0.333, 0.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.667, 0.0, 0.333, 0.667, 0.667, 0.667, 1.0, 0.0, 0.667, 0.333, 0.667, 0.5, 0.667, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.25, 1.25, 0.0, 0.667, 0.667, 0.333, 1.0, 0.0, 0.0, 0.667, 0.333, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 1.667, 0.333, 1.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.333, 1.0, 0.333, 0.667, 0.333, 0.0, 1.333, 1.333, 0.0, 0.0, 0.333, 0.333, 0.5, 0.333, 0.667, 0.333, 0.667, 0.0, 0.0, 0.0, 0.667, 1.0, 1.5, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.75, 0.75, 0.5, 0.25, 0.75, 1.0, 0.75, 0.5, 0.333, 0.0, 0.0, 0.0, 0.0, 0.846, 0.923, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.333, 0.0, 0.0, 0.333, 0.667, 1.0, 0.333, 0.333, 0.333, 0.333, 0.667, 0.667, 0.0, 0.5, 1.0, 0.0, 0.5, 0.5, 0.333, 0.0, 0.333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.667, 0.667, 0.667, 0.333, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 1.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.667, 0.0, 1.0, 0.5, 0.5, 0.5, 0.667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.125, 0.0, 0.0, 1.0, 0.25, 1.5, 0.0, 0.0, 1.0, 1.0, 0.667, 0.0, 1.0, 0.0, 0.333, 0.667, 1.0, 0.667, 0.667, 1.0, 1.0, 0.75, 0.5, 2.0, 1.5, 1.0, 1.0, 0.0, 1.0, 0.667, 0.667, 0.667, 0.333, 1.0, 0.0, 0.333, 0.5, 0.5, 0.0, 0.333, 0.333, 0.667, 0.0, 0.0, 0.667, 1.0, 0.667, 0.667, 0.0, 0.0, 1.0, 1.0, 0.667, 0.333, 0.0, 0.333, 0.333, 0.667, 0.111, 0.111, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.667, 1.0, 0.667, 0.5, 0.5, 0.5, 0.6, 0.667, 0.333, 0.333, 0.333, 1.0, 1.0, 0.0, 0.0, 0.5, 0.25, 0.667, 0.667, 0.333, 0.0, 0.333, 0.667, 0.667, 0.0, 1.0, 0.0, 0.333, 1.0, 0.667, 0.333, 0.0, 0.0, 0.667, 0.667, 0.333, 0.0, 1.0, 1.333, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.333, 1.0, 0.0, 0.667, 0.667, 0.333, 0.667, 0.333, 1.333, 1.0, 0.333, 0.0, 0.5, 0.667, 1.333, 0.333, 0.333, 0.0, 0.667, 1.0, 1.667, 0.667, 0.333, 0.333, 0.333, 1.0, 0.667, 1.333, 1.0, 0.667, 1.0, 1.333, 1.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.5, 0.667, 0.667, 0.333, 1.0, 0.0, 0.667, 1.0, 0.333, 0.667, 0.667, 1.0, 1.0, 0.667, 0.25, 1.0, 0.333, 0.0, 1.0, 0.667, 0.667, 0.333, 0.333, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.667, 0.5, 0.5, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.5, 0.667, 0.0, 0.0, 0.333, 0.667, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.333, 0.333, 1.0, 0.667, 0.333, 0.5, 0.0, 1.25, 1.0, 1.0, 0.333, 0.667, 0.333, 0.667, 1.0, 0.333, 1.0, 0.333, 0.667, 1.0, 1.0, 0.0, 0.667, 1.0, 0.667, 0.333, 0.333, 0.0, 0.0, 1.0, 0.667, 0.333, 0.667, 0.0, 0.0, 0.667, 0.333, 0.333, 0.0, 0.333, 1.0, 0.333, 0.333, 0.333, 0.667, 0.333, 0.333, 0.667, 0.333, 1.0, 0.667, 0.333, 0.333, 0.333, 0.667, 1.0, 1.0, 1.0, 1.0, 0.333, 1.0, 0.0, 0.667, 0.667, 1.0, 0.333, 0.667, 0.667, 0.0, 1.333, 0.5, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.667, 0.5, 0.75, 0.0, 0.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.667, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.333, 0.667, 0.0, 0.0, 0.0, 1.0, 0.0, 0.333, 0.0, 0.0, 0.333, 0.667, 0.667, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.333, 1.0, 0.667, 0.5, 0.5, 0.0, 0.5, 0.0, 0.667, 0.333, 0.667, 1.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.667, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 0.667, 0.333, 0.333, 0.0, 0.5, 0.5, 2.0, 1.5, 0.333, 0.333, 0.667, 0.667, 0.0, 0.333, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.667, 0.0, 0.667, 0.667, 0.667, 0.75, 0.25, 0.667, 0.0, 0.0, 0.5, 1.0, 0.667, 0.333, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.75, 0.25, 1.0, 1.0, 1.333, 1.0, 1.0, 0.667, 0.667, 0.0, 0.0, 1.0, 0.0, 0.25, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0, 0.667, 1.0, 0.0, 0.0, 1.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.6, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.333, 0.667, 0.333, 1.0, 1.5, 0.333, 0.0, 0.5, 0.0, 0.333, 0.333, 0.5, 0.0, 0.0, 0.667, 0.667, 0.0, 0.0, 0.75, 0.667, 0.0, 1.0, 0.667, 0.333, 0.667, 0.333, 1.0, 1.0, 0.5, 1.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 1.0, 0.0, 0.0, 0.667, 0.667, 0.667, 1.333, 0.0, 0.333, 0.333, 1.0, 0.0, 0.333, 1.0, 1.0, 0.333, 0.5, 0.75, 0.0, 0.0, 1.0, 0.333, 1.0, 1.0, 0.333, 0.0, 1.0, 0.5, 0.667, 1.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.333, 0.333, 0.333, 0.5, 0.0, 0.667, 0.333, 0.333, 0.667, 0.333, 0.333, 0.0, 0.0, 0.0, 0.333, 1.0, 1.0, 0.5, 0.5, 0.5, 0.0, 1.333, 0.333, 0.667, 0.0, 0.667, 0.667, 1.0, 0.0, 0.333, 0.667, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 1.0, 0.333, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.333, 0.0, 1.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.333, 0.5, 0.5, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.5, 0.0, 0.333, 0.0, 0.0, 0.0, 0.667, 1.0, 0.667, 0.727, 0.727, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.333, 0.0, 1.0, 0.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.333, 1.0, 0.667, 0.0, 0.667, 0.333, 0.5, 0.25, 0.333, 0.0, 0.0, 0.667, 0.333, 0.667, 0.333, 0.667, 1.5, 1.0, 0.333, 0.333, 0.0, 0.333, 0.0, 0.0, 0.667, 0.667, 0.667, 0.0, 0.333, 0.0, 0.667, 1.0, 0.0, 1.0, 0.0, 0.0, 0.667, 0.333, 0.333, 0.667, 0.333, 0.25, 0.0, 1.0, 0.333, 0.0, 0.0, 0.667, 1.0, 0.667, 0.636, 0.0, 0.667, 2.0, 1.0, 0.0, 1.0, 1.333, 0.667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.333, 0.333, 0.0, 0.0, 1.5, 0.0, 1.0, 1.0, 1.5, 0.0, 0.0, 0.0, 0.667, 0.0, 0.333, 0.333, 0.333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.333, 0.333, 0.0, 0.333, 0.667, 0.333, 0.667, 0.333, 0.333, 1.0, 0.5, 0.5, 1.0, 0.333, 0.333, 0.333, 0.667, 0.333, 0.0, 0.333, 0.75, 0.667, 0.333, 0.5, 0.667, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.333, 0.333, 0.0, 0.0, 1.0, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.667, 0.0, 0.333, 0.333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.75, 0.333, 0.333, 0.667, 0.667, 0.0, 0.667, 0.667, 0.0, 0.333, 0.667, 0.667, 0.333, 0.333, 0.75, 0.333, 0.333, 0.0, 0.0, 0.5, 0.333, 0.667, 1.333, 0.75, 0.667, 1.5, 0.0, 0.667, 0.667, 0.0, 0.667, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.5, 0.0, 0.0, 0.667, 0.0, 0.333, 0.667, 0.333, 1.0, 0.5, 0.0, 0.25, 0.333, 0.333, 0.0, 0.0, 1.0, 0.333, 0.333, 0.333, 0.667, 0.0, 0.333, 0.0, 0.0, 0.333, 0.667, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 1.0, 0.333, 0.0, 0.333, 0.0, 0.333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.667, 1.0, 0.0, 0.333, 1.0, 0.333, 0.0, 0.667, 1.0, 0.5, 0.5, 0.5, 0.5, 0.0, 0.2, 0.0, 1.333, 0.333, 0.0, 0.0, 2.0, 0.25, 0.667, 0.667, 0.0, 1.0, 0.5, 0.25, 0.5, 0.5, 1.0, 1.0, 0.7, 0.25, 0.5, 0.5, 0.667, 1.0, 0.0, 0.2, 0.8, 0.2, 0.25, 0.5, 0.333, 0.667, 0.333, 1.333, 0.333, 0.333, 0.667, 0.0, 0.667, 0.333, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.667, 1.0, 0.333, 0.667, 1.0, 1.0, 0.667, 0.667, 0.333, 0.667, 1.0, 0.5, 0.0, 0.667, 0.333, 0.0, 1.0, 0.0, 5.0, 0.0, 0.0, 0.333, 0.0, 0.25, 0.25, 0.75, 0.0, 0.25, 0.25, 0.5, 1.667, 0.0, 0.667, 0.667, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.333, 0.5, 1.5, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.5, 1.0, 0.0, 0.667, 1.5, 1.0, 1.5, 0.333, 0.667, 1.667, 1.0, 0.25, 0.75, 0.75, 1.5, 0.5, 0.667, 1.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.333, 0.0, 0.5, 0.667, 0.333, 0.0, 0.0, 0.5, 0.0, 0.667, 0.25, 0.333, 0.667, 0.0, 0.0, 0.333, 0.667, 1.0, 0.0, 1.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.333, 0.667, 1.0, 0.667, 0.667, 0.25, 1.0, 0.0, 0.667, 0.0, 0.75, 0.0, 0.333, 1.0, 2.0, 1.0, 0.0, 0.667, 0.667, 0.0, 1.0, 0.667, 0.667, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.667, 0.0, 0.333, 0.222, 2.0, 1.0, 0.667, 0.333, 0.0, 1.0, 0.333, 0.5, 0.0, 1.0, 0.5, 0.75, 0.667, 1.0, 0.333, 1.0, 1.0, 0.667, 1.0, 0.333, 0.333, 0.333, 0.0, 0.667, 0.333, 0.5, 0.0, 0.5, 0.429, 0.5, 1.0, 1.5, 1.0, 1.0, 0.667, 0.667, 0.0, 0.5, 0.0, 1.0, 0.5, 0.5, 0.5, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 0.5, 0.0, 0.25, 0.25, 2.25, 1.0, 0.333, 1.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.333, 0.333, 0.0, 0.667, 1.333, 0.5, 0.0, 0.333, 0.333, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.333, 1.0, 0.0, 0.333, 0.333, 0.5, 0.5, 0.0, 0.333, 0.0, 0.5, 1.5, 0.0, 1.5, 0.25, 0.0, 0.5, 0.333, 0.571, 1.143, 0.857, 0.714, 0.857, 0.0, 0.667, 0.0, 0.5, 1.0, 0.5, 1.5, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.667, 1.333, 0.333, 0.0, 0.333, 1.0, 1.0, 1.333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.333, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.667, 0.333, 0.0, 0.5, 0.0, 1.0, 2.0, 1.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 0.0, 1.0, 0.5, 0.25, 0.333, 0.333, 0.667, 0.0, 0.333, 1.0, 0.333, 0.333, 0.0, 0.0, 0.333, 0.333, 0.0, 1.333, 1.333, 1.0, 1.0, 0.0, 0.333, 0.667, 1.0, 0.667, 0.0, 0.0, 0.5, 0.0, 0.333, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.667, 0.0, 0.0, 0.0, 0.667, 1.0, 1.0, 0.0, 0.333, 0.667, 0.333, 0.0, 1.0, 1.333, 0.667, 1.0, 0.667, 0.0, 0.0, 1.333, 0.333, 0.0, 0.0, 0.0, 0.0, 1.0, 0.667, 0.667, 0.0, 0.333, 0.0, 0.5, 0.5, 0.0, 1.0, 1.5, 0.333, 0.333, 0.333, 0.0, 0.667, 1.0, 0.333, 1.0, 0.333, 0.667, 0.667, 0.667, 0.0, 1.0, 0.333, 0.0, 0.667, 0.333, 0.0, 0.333, 0.667, 0.333, 0.0, 0.667, 0.0, 0.667, 0.333, 0.333, 1.5, 1.0, 0.333, 0.333, 0.0, 0.667, 0.667, 0.333, 0.333, 1.0, 0.0, 0.333, 0.333, 0.667, 0.0, 0.667, 0.667, 0.667, 0.75, 1.0, 0.667, 0.667, 0.333, 0.333, 0.0, 0.0, 0.667, 0.0, 0.0, 0.333, 0.0, 0.0, 0.667, 0.0, 0.0, 0.25, 0.75, 0.333, 0.333, 0.667, 0.889, 0.0, 0.667, 0.0, 0.75, 0.75, 0.5, 0.25, 0.333, 0.333, 0.0, 0.0, 1.0, 0.333, 0.0, 0.0, 0.667, 0.333, 0.0, 0.333, 0.0, 0.333, 0.0, 0.333, 0.667, 0.333, 0.714, 0.857, 0.75, 0.667, 0.667, 0.667, 0.333, 1.0, 1.0, 0.667, 0.667, 0.333, 0.667, 0.0, 0.333, 0.333, 0.0, 0.5, 0.75, 0.0, 1.0, 0.5, 0.333, 0.667, 1.0, 0.667, 0.667, 0.0, 1.0, 0.0, 0.667, 0.667, 0.333, 1.0, 0.333, 0.333, 0.667, 0.0, 1.0, 0.0, 0.0, 0.333, 0.0, 0.5, 0.5, 0.333, 0.333, 0.667, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.333, 0.333, 0.5, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.5, 1.0, 1.0, 0.667, 1.0, 1.5, 1.5, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.333, 1.0, 0.667, 0.333, 0.667, 0.667, 1.0, 1.0, 0.5, 1.0, 0.667, 0.667, 0.923, 0.667, 1.0, 0.333, 0.333, 1.0, 0.667, 0.667, 0.667, 1.0, 0.333, 1.333, 0.333, 0.0, 0.2, 0.2, 0.333, 0.333, 0.667, 0.0, 0.0, 0.5, 0.333, 1.0, 1.0, 0.667, 0.333, 0.5, 0.5, 0.5, 0.5, 0.75, 0.667, 1.0, 0.5, 1.333, 0.333, 0.333, 0.667, 0.5, 1.5, 0.667, 0.667, 0.667, 0.667, 0.667, 0.667, 1.333, 0.333, 0.667, 1.0, 0.0, 0.667, 0.333, 0.0, 0.0, 0.667, 0.667, 0.0, 0.667, 1.0, 1.333, 0.667, 1.0, 0.667, 0.667, 1.0, 1.0, 0.333, 0.0, 0.5, 0.5, 0.5, 0.333, 0.0, 0.333, 1.0, 0.333, 1.333, 0.667, 0.333, 0.222, 1.0, 0.667, 0.667, 1.0, 0.333, 0.0, 0.0, 1.0, 0.333, 0.667, 0.333, 0.667, 0.667, 0.333, 0.667, 0.333, 1.0, 0.25, 0.0, 0.75, 0.0, 0.333, 0.75, 1.0, 0.333, 0.0, 0.0, 0.0, 1.0, 0.333, 1.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.333, 0.25, 0.25, 0.333, 1.333, 0.333, 0.333, 0.667, 0.667, 0.0, 0.5, 1.0, 0.5, 0.667, 0.333, 1.0, 1.0, 0.636, 0.909, 0.333, 1.0, 1.0, 1.0, 0.333, 0.667, 0.5, 0.333, 1.333, 1.0, 0.667, 0.667, 0.0, 0.0, 0.667, 0.0, 0.0, 0.25, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.75, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.5, 0.0, 0.0, 0.0, 1.5, 0.333, 0.667, 0.667, 0.667, 1.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.333, 1.0, 0.0, 0.667, 0.333, 1.333, 0.667, 0.0, 0.0, 0.0, 0.333, 1.333, 0.0, 0.0, 0.333, 0.667, 0.0, 0.667, 0.333, 0.333, 0.333, 0.0, 0.333, 0.0, 0.0, 0.5, 0.5, 0.5, 0.0, 0.0, 0.0, 0.333, 0.0, 1.0, 0.667, 0.667, 0.667, 0.667, 0.667, 0.667, 0.333, 0.5, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.333, 0.0, 0.286, 1.0, 0.0, 0.333, 0.667, 0.667, 1.0, 1.0, 0.667, 0.333, 0.5, 0.25, 0.25, 0.0, 0.5, 0.5, 0.333, 0.0, 1.0, 1.0, 0.667, 0.667, 1.0, 0.667, 0.0, 0.333, 0.667, 1.0, 1.0, 1.333, 0.333, 0.333, 0.0, 0.667, 0.333, 0.667, 0.333, 1.0, 0.5, 0.0, 0.0, 0.0, 0.636, 0.667, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.5, 0.5, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 2.0, 0.0, 0.333, 0.5, 0.75, 0.0, 0.333, 1.0, 1.0, 0.5, 0.667, 0.333, 0.333, 0.667, 0.0, 0.667, 0.5, 0.75, 0.5, 0.333, 0.0, 0.25, 0.75, 0.5, 0.667, 0.5, 1.5, 0.5, 0.5, 1.0, 0.25, 0.0, 1.0, 1.0, 0.333, 0.5, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.333, 1.0, 1.0, 0.667, 0.333, 0.667, 0.333, 0.333, 0.333, 1.5, 1.0, 1.0, 1.5, 0.333, 0.667, 0.0, 0.5, 0.0, 0.667, 0.5, 0.0, 0.0, 1.0, 1.0, 0.333, 0.333, 0.333, 0.667, 1.0, 0.667, 1.333, 1.0, 0.333, 1.0, 0.5, 0.333, 1.333, 0.333, 1.222, 0.0, 0.667, 0.667, 0.667, 3.333, 1.5, 0.667, 1.0, 0.333, 1.0, 1.0, 0.75, 0.25, 0.25, 0.5, 0.25, 1.333, 0.333, 0.333, 0.667, 1.0, 0.667, 0.333, 0.333, 0.333, 0.5, 0.333, 1.0, 0.0, 1.0, 0.667, 0.667, 0.667, 0.5, 0.0, 0.0, 0.333, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.5, 0.667, 0.0, 0.0, 0.333, 0.0, 0.333, 0.25, 0.0, 0.0, 1.0, 0.5, 0.0, 0.667, 1.0, 1.333, 1.0, 0.5, 0.5, 0.333, 0.667, 1.0, 0.5, 0.0, 0.0, 0.333, 0.667, 0.667, 0.0, 0.5, 0.333, 0.0, 1.0, 1.5, 0.5, 0.0, 1.5, 1.5, 0.75, 0.5, 1.0, 0.0, 0.667, 0.5, 1.0, 1.0, 0.333, 0.0, 0.333, 0.333, 0.667, 0.667, 0.0, 0.333, 1.0, 1.0, 1.0, 1.0, 1.333, 1.0, 0.667, 0.5, 1.0, 0.667, 1.0, 1.0, 0.0, 1.0, 1.0, 1.5, 1.667, 0.667, 0.5, 0.5, 0.0, 1.0, 0.636, 0.333, 0.5, 0.667, 0.0, 0.0, 0.5, 0.0, 0.5, 0.333, 1.0, 1.0, 1.5, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.667, 1.0, 0.333, 1.5, 1.5, 0.5, 1.5, 0.5, 0.667, 0.0, 0.667, 1.0, 0.333, 0.333, 1.0, 1.0, 1.0, 0.333, 1.333, 0.667, 0.333, 1.0, 0.0, 0.333, 1.5, 1.5, 1.5, 1.0, 0.667, 0.5, 0.0, 0.667, 1.0, 1.0, 0.333, 0.0, 0.25, 0.333, 0.0, 0.667, 1.0, 1.0, 0.5, 0.5, 0.333, 0.333, 0.333, 0.333, 1.5, 0.0, 0.0, 0.0, 0.5, 0.667, 0.0, 0.333, 1.0, 0.667, 1.0, 0.333, 0.333, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.333, 0.333, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.667, 0.667, 0.0, 0.0, 0.333, 0.667, 1.0, 1.0, 1.333, 0.333, 0.9, 1.4, 0.667, 0.667, 0.0, 0.667, 0.0, 0.0, 0.667, 1.333, 1.0, 0.667, 0.0, 0.0, 0.0, 1.0, 0.667, 1.0, 1.5, 1.0, 1.0, 0.333, 0.333, 1.0, 0.667, 0.667, 0.0, 0.0, 0.667, 1.0, 0.667, 0.0, 0.0, 0.667, 1.0, 1.0, 1.0, 0.5, 1.5, 0.667, 0.333, 1.0, 0.667, 0.667, 0.333, 0.333, 0.667, 1.0, 0.0, 0.667, 0.333, 1.0, 0.333, 0.667, 1.0, 1.333, 1.5, 1.0, 0.667, 0.333, 0.333, 0.333, 1.0, 0.667, 0.0, 1.333, 1.0, 0.667, 0.333, 0.667, 1.0, 1.0, 0.5, 0.333, 0.333, 1.0, 1.0, 1.0, 1.0, 0.25, 0.667, 0.333, 1.0, 0.333, 0.0, 0.667, 0.667, 0.0, 0.5, 0.0, 0.5, 0.333, 0.667, 0.333, 0.333, 1.5, 0.0, 1.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 1.0, 0.0, 1.5, 1.0, 0.0, 1.5, 0.667, 0.667, 0.333, 0.0, 1.0, 0.0, 1.0, 1.5, 0.75, 0.333, 0.0, 1.0, 0.667, 0.333, 0.0, 0.667, 0.333, 0.0, 0.333, 0.0, 0.667, 0.0, 0.0, 0.333, 1.0, 0.333, 0.333, 0.0, 0.667, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 0.25, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.667, 0.667, 0.667, 0.667, 0.5, 0.0, 0.0, 1.0, 1.0, 0.667, 0.0, 0.667, 0.333, 0.667, 1.5, 1.5, 0.333, 0.333, 0.333, 1.5, 0.0, 0.667, 0.0, 0.667, 0.0, 0.333, 0.333, 0.333, 0.333, 0.333, 1.0, 1.0, 1.333, 1.0, 0.5, 0.0, 0.333, 0.333, 0.333, 0.333, 1.0, 0.0, 2.0, 0.5, 0.667, 0.0, 1.333, 0.667, 0.667, 0.333, 0.333, 1.0, 0.5, 0.5, 1.5, 0.5, 1.5, 0.75, 0.333, 0.5, 0.0, 0.333, 0.333, 0.0, 0.333, 1.0, 0.667, 0.5, 0.5, 1.0, 0.333, 1.0, 1.0, 0.333, 0.667, 0.333, 0.333, 0.333, 0.333, 0.333, 0.5, 1.0, 0.333, 0.333, 1.0, 0.7, 0.333, 0.333, 0.667, 2.0, 1.0, 0.25, 0.75, 0.667, 0.0, 0.5, 0.5, 1.333, 1.143, 0.5, 0.5, 0.5, 0.25, 0.333, 0.667, 1.0, 1.0, 0.0, 0.0, 0.333, 0.333, 0.667, 1.0, 0.75, 0.25, 1.0, 1.0, 0.667, 0.0, 0.667, 0.667, 1.0, 0.333, 0.667, 1.0, 0.333, 0.667, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.5, 0.667, 1.333, 1.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.333, 0.0, 0.667, 0.333, 0.333, 0.0, 0.333, 1.0, 1.0, 0.0, 0.667, 0.0, 1.0, 0.333, 1.333, 0.667, 0.5, 0.5, 0.8, 0.8, 1.6, 0.25, 0.0, 0.0, 1.0, 0.667, 1.0, 0.5, 0.333, 1.0, 0.0, 0.667, 0.0, 0.667, 0.0, 0.0, 0.667, 1.0, 0.0, 0.0, 0.75, 0.25, 0.75, 0.25, 0.0, 0.333, 0.0, 0.0, 0.333, 0.333, 0.667, 1.0, 0.75, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.667, 0.333, 0.333, 0.333, 0.333, 0.667, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.333, 0.333, 0.0, 0.0, 0.5, 0.667, 0.0, 0.667, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.75, 0.667, 0.667, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.667, 0.0, 0.0, 0.0, 0.0, 1.0, 0.667, 0.667, 0.667, 0.0, 0.333, 0.0, 1.5, 1.0, 0.0, 0.5, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.667, 0.667, 1.0, 1.0, 0.667, 0.333, 0.333, 0.667, 0.667, 0.333, 0.333, 0.333, 0.667, 0.333, 0.333, 1.0, 1.333, 0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.667, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.5, 0.333, 0.667, 0.667, 0.333, 0.0, 1.0, 0.333, 1.0, 0.0, 1.0, 0.667, 0.333, 1.0, 0.5, 1.0, 1.0, 1.5, 0.667, 0.5, 0.5, 0.333, 1.0, 0.667, 0.667, 0.5, 0.333, 0.333, 0.0, 0.333, 0.333, 1.0, 0.667, 0.0, 0.0, 1.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.333, 0.333, 0.667, 0.333, 0.333, 0.333, 0.667, 0.333, 0.333, 0.0, 0.0, 0.538, 0.5, 0.5, 0.0, 0.667, 0.667, 1.0, 1.0, 1.5, 1.0, 0.667, 0.667, 0.333, 0.333, 0.667, 1.0, 1.0, 0.667, 0.0, 1.0, 1.333, 0.643, 0.667, 0.667, 0.667, 1.0, 0.0, 1.5, 1.0, 0.25, 0.444, 1.0, 1.0, 1.0, 0.333, 0.875, 1.0, 1.5, 0.556, 0.0, 0.5, 0.667, 1.333, 0.667, 0.667, 0.667, 0.0, 0.333, 0.667, 0.0, 0.667, 0.667, 1.0, 0.333, 1.0, 0.667, 0.25, 1.0, 1.0, 0.5, 0.0, 0.5, 0.333, 1.5, 0.333, 0.667, 1.0, 1.0, 0.0, 0.667, 0.0, 1.0, 0.333, 0.667, 1.0, 1.0, 0.333, 1.0, 0.667, 0.333, 0.667, 0.333, 0.333, 0.333, 0.333, 1.0, 0.333, 1.0, 0.0, 0.333, 0.667, 0.333, 1.333, 1.0, 0.0, 0.667, 0.333, 0.333, 0.0, 0.333, 1.0, 0.333, 0.0, 0.333, 0.0, 1.0, 0.0, 0.0, 0.667, 1.0, 0.0, 0.667, 0.0, 0.667, 0.333, 1.0, 0.5, 1.0, 0.667, 0.333, 0.667, 0.333, 1.333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.667, 0.667, 0.25, 0.0, 0.5, 0.333, 0.0, 0.0, 0.333, 1.0, 0.667, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.333, 1.0, 0.5, 0.5, 0.333, 0.0, 0.0, 1.0, 1.333, 1.333, 0.667, 0.0, 0.0, 0.667, 0.0, 0.333, 0.333, 1.0, 0.5, 0.0, 1.0, 0.667, 1.0, 1.0, 0.0, 0.5, 0.5, 0.5, 0.0, 0.5, 0.667, 0.0, 0.333, 0.333, 0.5, 0.5, 0.0, 0.5, 0.25, 0.0, 0.667, 0.333, 0.0, 0.667, 1.0, 0.333, 0.75, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.667, 1.0, 0.333, 1.0, 0.0, 0.5, 0.5, 0.333, 1.0, 1.0, 0.667, 0.667, 1.0, 0.333, 0.0, 0.0, 0.333, 0.333, 0.333, 0.333, 0.25, 1.0, 0.333, 0.333, 0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.75, 1.0, 0.667, 0.667, 0.667, 0.333, 0.5, 1.0, 1.0, 1.0, 0.5, 0.25, 0.25, 0.5, 0.5, 0.667, 0.333, 1.0, 0.333, 0.333, 0.333, 0.333, 0.667, 0.0, 1.5, 0.0, 0.0, 0.5, 0.5, 0.0, 0.25, 0.0, 0.0, 0.667, 0.0, 0.333, 0.667, 0.0, 0.0, 0.333, 0.333, 1.0, 0.6, 0.333, 0.0, 0.667, 1.0, 0.667, 0.667, 0.25, 0.25, 1.0, 0.667, 0.667, 0.333, 0.0, 0.667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.667, 0.333, 0.667, 0.0, 0.0, 0.333, 0.0, 0.5, 0.0, 0.667, 1.0, 0.333, 1.0, 0.5, 0.667, 1.0, 0.75, 0.25, 0.0, 0.25, 0.0, 0.25, 1.0, 0.333, 0.667, 0.333, 1.0, 2.667, 0.333, 0.667, 1.0, 0.0, 0.0, 1.333, 0.0, 1.0, 0.333, 0.0, 0.0, 0.5, 0.667, 0.0, 0.0, 0.5, 0.333, 0.0, 0.667, 0.333, 0.0, 0.0, 0.0, 0.5, 0.667, 0.333, 1.0, 1.0, 1.0, 1.857, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.25, 1.0, 0.667, 1.0, 0.667, 0.333, 1.333, 0.5, 1.0, 1.0, 1.0, 1.0, 0.667, 0.667, 1.5, 1.0, 0.0, 0.5, 0.333, 0.667, 0.333, 0.667, 0.667, 0.667, 0.333, 0.333, 0.667, 0.667, 0.667, 1.0, 0.0, 0.667, 0.333, 0.333, 0.333, 1.0, 1.667, 0.0, 1.75, 1.25, 0.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.0, 0.5, 0.667, 1.0, 0.333, 0.333, 0.5, 0.333, 0.5, 0.5, 0.5, 1.0, 1.0, 1.5, 0.5, 1.5, 1.0, 1.0, 0.5, 0.0, 0.333, 0.0, 0.333, 0.333, 1.0, 0.333, 0.333, 0.75, 0.25, 1.0, 0.333, 1.0, 0.333, 0.0, 0.333, 1.333, 1.0, 0.667, 0.0, 0.333, 0.333, 0.667, 1.0, 0.667, 0.333, 0.333, 0.875, 0.667, 0.333, 0.667, 0.667, 0.667, 0.0, 0.667, 1.0, 0.0, 1.0, 0.667, 0.0, 0.0, 0.667, 0.667, 0.333, 0.0, 1.0, 0.667, 0.667, 1.0, 0.25, 0.25, 0.0, 0.333, 0.0, 0.5, 0.75, 1.0, 0.667, 0.667, 1.0, 1.0, 1.0, 1.0, 0.667, 0.5, 0.0, 0.333, 0.0, 0.667, 0.333, 0.0, 0.0, 0.667, 0.333, 1.0, 0.5, 1.0, 1.0, 0.5, 0.333, 0.667, 0.333, 0.333, 0.667, 0.333, 0.333, 0.667, 1.0, 0.333, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.333, 0.0, 1.0, 0.0, 0.333, 0.333, 1.5, 1.0, 0.333, 0.667, 0.667, 0.333, 0.333, 1.0, 0.667, 0.667, 0.333, 0.0, 1.0, 0.0, 0.333, 1.0, 1.333, 1.0, 0.667, 1.0, 1.0, 0.667, 0.0, 0.667, 1.333, 0.0, 0.0, 0.333, 0.667, 0.25, 1.0, 0.667, 0.333, 1.0, 0.667, 0.0, 1.333, 1.0, 0.667, 0.667, 0.667, 0.333, 0.667, 1.0, 1.0, 0.667, 0.0, 0.0, 0.0, 0.667, 1.0, 1.333, 1.333, 1.0, 0.333, 0.333, 0.333, 0.333, 1.0, 1.5, 0.5, 0.0, 1.0, 1.0, 1.333, 0.333, 0.667, 1.333, 1.0, 0.5, 1.0, 0.333, 1.0, 0.333, 0.667, 0.333, 0.333, 0.667, 0.667, 1.0, 0.333, 0.667, 0.5, 1.0, 0.0, 0.667, 0.667, 0.667, 0.667, 0.333, 0.0, 0.667, 1.0, 0.5, 0.667, 0.333, 0.333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.583, 1.0, 0.667, 0.667, 0.0, 0.333, 0.333, 0.333, 0.333, 0.667, 0.0, 0.667, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.923, 0.0, 0.0, 0.0, 0.0, 0.75, 0.25, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.667, 1.0, 1.0, 1.0, 0.5, 1.5, 0.5, 0.0, 0.333, 1.0, 0.667, 0.333, 0.778, 0.8, 1.333, 0.0, 1.333, 0.75, 1.0, 0.75, 1.0, 1.0, 0.75, 0.25, 0.25, 0.667, 0.333, 0.0, 1.0, 0.333, 0.333, 0.333, 0.5, 0.0, 0.0, 1.0, 1.5, 1.0, 0.5, 0.333, 0.667, 0.667, 0.0, 1.0, 1.333, 0.667, 0.667, 1.5, 0.667, 0.667, 0.333, 0.667, 0.333, 1.0, 0.667, 1.0, 0.667, 1.333, 1.0, 0.667, 0.5, 0.5, 0.0, 0.667, 1.0, 0.333, 0.667, 0.333, 0.5, 0.75, 0.25, 0.5, 0.5, 0.5, 0.5, 0.333, 0.333, 1.0, 0.5, 0.0, 1.111, 0.667, 0.667, 0.667, 1.333, 0.667, 0.667, 0.75, 1.0, 0.333, 0.0, 1.0, 0.667, 1.5, 0.0, 0.5, 0.5, 1.0, 1.0, 0.667, 0.667, 0.667, 0.667, 0.0, 0.333, 0.25, 0.25, 1.0, 0.0, 0.667, 0.333, 0.333, 0.333, 0.333, 0.667, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.333, 0.333, 0.667, 0.333, 0.75, 0.5, 0.0, 0.667, 0.0, 0.333, 1.0, 1.0, 1.0, 0.667, 0.667, 0.333, 1.0, 1.667, 0.667, 1.0, 0.667, 0.0, 0.333, 0.333, 0.333, 0.333, 0.667, 0.333, 0.333, 0.333, 0.0, 0.0, 0.333, 0.444, 1.0, 1.0, 1.0, 0.667, 0.0, 0.333, 1.333, 1.0, 0.667, 1.0, 1.5, 1.5, 1.0, 0.333, 1.0, 0.333, 1.0, 1.0, 0.667, 0.333, 0.0, 0.333, 0.0, 0.667, 1.0, 0.0, 0.333, 0.667, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.333, 0.0, 0.0, 1.0, 0.667, 0.333, 0.333, 0.667, 1.0, 1.0, 1.0, 0.333, 0.667, 0.333, 1.0, 0.667, 0.5, 1.0, 1.0, 1.0, 0.333, 0.0, 0.667, 0.667, 0.667, 1.0, 1.0, 0.667, 0.0, 0.667, 0.333, 0.667, 0.0, 0.667, 0.0, 0.667, 0.0, 1.0, 0.333, 0.333, 0.0, 0.333, 0.5, 0.0, 0.5, 0.5, 0.333, 0.667, 0.333, 0.333, 0.667, 1.0, 1.333, 1.0, 0.333, 0.333, 0.333, 1.0, 0.0, 0.0, 0.5, 0.75, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.5, 0.0, 0.0, 0.5, 0.333, 0.333, 0.333, 0.333, 0.25, 0.5, 0.667, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.333, 0.0, 1.0, 1.0, 0.667, 1.0, 0.0, 0.667, 0.667, 0.0, 0.667, 1.0, 1.0, 1.0, 1.0, 0.333, 0.333, 0.333, 0.667, 0.667, 0.333, 0.667, 1.0, 0.667, 1.0, 0.25, 0.333, 0.667, 0.667, 0.333, 0.0, 0.333, 0.0, 0.333, 0.667, 1.333, 1.0, 0.333, 0.0, 0.333, 0.333, 0.333, 1.0, 0.75, 1.0, 0.667, 0.333, 1.0, 0.333, 0.667, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.667, 0.333, 0.667, 0.667, 0.667, 0.333, 0.333, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 1.5, 1.0, 1.0, 1.0, 0.333, 0.333, 0.667, 0.667, 1.0, 0.667, 0.0, 0.333, 0.333, 1.0, 0.5, 0.667, 0.333, 0.5, 0.667, 0.667, 0.667, 1.0, 0.0, 0.333, 0.333, 1.0, 1.0, 0.667, 0.333, 1.3, 0.333, 1.0, 0.5, 0.667, 0.667, 1.0, 0.333, 0.0, 1.0, 0.5, 1.0, 0.25, 0.5, 0.0, 0.333, 0.0, 0.333, 1.0, 0.333, 0.0, 0.667, 0.667, 1.0, 0.5, 0.0, 0.333, 1.333, 0.667, 0.0, 0.333, 0.667, 0.333, 0.333, 0.667, 0.667, 1.0, 1.0, 1.0, 0.667, 0.667, 0.5, 0.5, 1.0, 1.0, 0.5, 1.5, 0.5, 1.0, 0.667, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 2.5, 0.0, 0.333, 0.333, 0.667, 0.333, 1.0, 1.0, 0.667, 0.0, 0.333, 0.0, 0.333, 1.5, 0.5, 1.333, 1.0, 1.0, 0.0, 0.667, 0.333, 0.727, 0.0, 0.333, 0.333, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.667, 0.667, 1.0, 0.667, 1.5, 0.5, 1.0, 1.0, 1.0, 0.333, 0.667, 0.333, 1.0, 0.333, 1.0, 1.0, 0.333, 0.333, 1.0, 0.333, 0.0, 0.5, 0.667, 0.0, 0.667, 0.333, 0.333, 0.667, 0.667, 0.0, 1.0, 0.333, 0.333, 0.667, 0.333, 1.5, 1.5, 0.0, 0.333, 0.0, 0.0, 0.5, 0.0, 1.0, 0.667, 0.333, 1.0, 1.0, 0.333, 0.5, 0.5, 0.333, 0.333, 1.0, 0.667, 0.667, 0.333, 0.333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.333, 0.667, 1.333, 0.333, 1.333, 1.333, 1.0, 1.0, 0.333, 0.667, 0.625, 0.667, 1.0, 1.5, 1.0, 0.333, 0.667, 0.333, 0.667, 1.0, 1.333, 1.0, 1.333, 0.667, 0.333, 0.0, 0.667, 0.667, 0.333, 0.333, 0.0, 0.667, 0.0, 0.667, 1.0, 0.667, 0.0, 0.667, 0.0, 1.0, 0.667, 0.0, 0.333, 1.0, 0.333, 0.667, 0.0, 0.0, 0.5, 0.333, 0.667, 0.333, 0.333, 0.333, 0.333, 0.333, 0.0, 0.667, 1.0, 1.0, 0.333, 0.667, 0.333, 1.5, 0.333, 1.0, 1.0, 0.0, 1.333, 1.0, 0.667, 1.0, 0.667, 0.5, 0.5, 0.5, 0.5, 0.0, 0.333, 0.333, 0.0, 0.0, 1.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.667, 0.667, 0.0, 0.0, 0.0, 0.75, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 0.667, 0.5, 0.0, 1.0, 0.5, 0.5, 0.5, 0.25, 0.0, 0.333, 0.333, 1.0, 1.0, 0.0, 0.667, 0.667, 1.333, 0.333, 1.0, 0.333, 0.0, 0.333, 0.0, 0.333, 0.333, 0.333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.667, 0.333, 0.667, 0.0, 0.667, 1.0, 0.667, 0.75, 0.5, 0.333, 1.667, 0.333, 0.5, 0.5, 0.875, 0.667, 0.333, 0.333, 0.0, 1.0, 0.5, 0.25, 0.5, 0.5, 1.0, 0.5, 0.5, 0.0, 0.0, 1.0, 0.333, 0.333, 0.667, 1.0, 1.5, 0.333, 0.0, 0.667, 0.333, 0.0, 0.333, 0.0, 0.667, 0.0, 0.333, 0.0, 0.5, 0.0, 1.0, 0.5, 0.333, 0.5, 0.25, 1.0, 0.0, 0.0, 1.5, 1.0, 1.0, 0.75, 0.75, 0.75, 0.0, 1.5, 0.0, 0.5, 0.0, 0.5, 0.667, 1.0, 1.0, 0.667, 0.667, 0.5, 0.5, 0.5, 0.333, 0.333, 0.333, 0.0, 0.333, 0.333, 0.667, 0.333, 1.0, 1.0, 0.615, 0.333, 0.333, 1.0, 0.0, 0.667, 4.667, 0.333, 0.333, 0.333, 1.182, 0.333, 0.333, 1.333, 0.5, 0.0, 0.0, 0.0, 0.667, 0.667, 0.0, 0.333, 0.0, 0.0, 0.667, 0.667, 0.5, 0.0, 1.0, 1.5, 0.333, 1.0, 1.0, 1.5, 2.0, 0.25, 0.25, 1.0, 0.667, 0.0, 0.0, 0.333, 0.0, 0.667, 0.333, 0.333, 0.667, 0.0, 0.333, 0.0, 1.0, 1.0, 0.667, 0.333, 0.0, 0.5, 1.5, 1.5, 0.667, 1.0, 0.667, 1.0, 0.667, 1.0, 1.0, 1.0, 0.0, 1.667, 0.0, 1.0, 0.75, 0.0, 0.667, 0.667, 0.0, 0.75, 0.5, 0.0, 0.667, 0.667, 0.667, 1.0, 0.333, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.5, 1.5, 0.5, 0.333, 0.667, 0.667, 0.333, 0.333, 0.667, 0.333, 1.0, 1.0, 0.667, 0.333, 0.667, 0.667, 0.5, 0.333, 1.333, 0.0, 1.0, 1.5, 0.0, 0.5, 0.333, 0.75, 1.0, 1.667, 0.333, 0.333, 0.5, 0.5, 0.667, 0.667, 1.0, 1.0, 0.333, 1.0, 0.333, 0.333, 1.333, 1.0, 0.667, 0.667, 0.333, 0.667, 1.0, 1.0, 0.667, 1.0, 0.667, 1.0, 0.5, 1.0, 0.0, 0.333, 0.667, 0.0, 0.0, 1.667, 0.0, 0.5, 0.667, 0.667, 1.0, 0.0, 1.0, 0.0, 0.667, 1.0, 1.0, 0.667, 1.0, 0.0, 0.333, 0.25, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.667, 0.333, 0.333, 0.0, 0.667, 0.0, 0.667, 0.25, 1.0, 1.333, 1.0, 0.667, 0.0, 1.0, 0.667, 1.333, 1.5, 1.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.5, 0.333, 0.0, 0.667, 0.667, 0.333, 0.333, 0.0, 0.667, 0.0, 0.333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.667, 0.333, 0.0, 0.0, 0.333, 0.0, 0.0, 0.5, 0.667, 1.0, 0.75, 0.333, 0.333, 1.0, 1.5, 1.333, 0.333, 0.0, 0.667, 0.0, 0.0, 0.0, 0.667, 0.0, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 1.0, 0.0, 0.5, 0.667, 0.333, 1.333, 0.333, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.333, 0.333, 0.667, 0.333, 0.0, 0.667, 0.0, 0.0, 0.667, 0.667, 0.667, 1.0, 0.0, 0.333, 0.0, 0.333, 0.667, 0.333, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.5, 1.0, 0.5, 0.5, 0.667, 0.667, 0.667, 0.0, 0.0, 0.75, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.333, 0.333, 0.333, 1.0, 0.5, 0.5, 0.0, 0.5, 0.0, 0.0, 0.333, 0.333, 0.0, 0.333, 0.333, 0.333, 1.0, 0.667, 0.667, 0.667, 0.333, 0.333, 0.333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 1.0, 0.333, 0.667, 0.0, 0.333, 0.333, 0.667, 0.333, 0.333, 0.0, 0.5, 0.0, 0.333, 0.667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.333, 0.0, 0.333, 1.0, 0.0, 0.667, 0.333, 0.667, 0.0, 0.667, 0.333, 0.333, 0.0, 0.0, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 1.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.667, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.333, 0.333, 0.0, 0.667, 0.333, 0.0, 0.0, 0.667, 0.0, 0.333, 0.0, 0.0, 0.0, 1.0, 1.0, 0.667, 0.333, 0.667, 1.0, 0.333, 0.0, 0.0, 0.333, 0.0, 1.0, 0.0, 0.25, 0.25, 0.25, 0.0, 0.75, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.333, 0.667, 0.667, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 0.667, 0.667, 0.667, 0.667, 1.0, 0.667, 0.0, 1.333, 0.25, 0.5, 0.75, 0.333, 0.0, 0.667, 0.667, 0.0, 0.0, 1.0, 0.667, 0.0, 0.0, 0.667, 2.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.333, 0.0, 0.0, 0.5, 0.0, 1.0, 0.333, 0.667, 1.667, 0.0, 0.0, 0.667, 0.667, 0.333, 0.0, 0.667, 0.5, 0.667, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.667, 0.333, 0.667, 0.333, 0.667, 0.667, 0.333, 0.667, 0.667, 0.667, 0.0, 0.667, 0.667, 0.333, 0.0, 0.0, 1.333, 1.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 1.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.333, 0.0, 1.667, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.333, 0.333, 0.667, 0.333, 0.667, 0.667, 0.0, 0.0, 0.0, 0.667, 0.667, 0.0, 0.0, 0.333, 0.333, 1.0, 0.667, 0.0, 0.333, 0.0, 0.5, 0.5, 0.25, 0.0, 0.333, 0.667, 0.667, 0.5, 0.0, 1.0, 0.0, 0.5, 1.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.667, 0.0, 1.0, 0.667, 0.0, 0.333, 1.0, 0.667, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.667, 0.333, 0.0, 0.0, 0.0, 1.0, 0.667, 0.333, 0.667, 0.0, 0.667, 0.667, 0.0, 0.0, 0.0, 0.5, 0.5, 0.75, 0.25, 0.5, 0.333, 0.0, 0.667, 0.0, 0.0, 0.333, 0.333, 0.0, 0.667, 0.667, 0.0, 0.667, 0.667, 1.0, 1.0, 2.0, 0.0, 1.333, 0.333, 0.0, 0.0, 0.833, 0.667, 0.333, 0.667, 1.0, 1.0, 0.667, 1.0, 0.667, 0.333, 0.0, 0.667, 0.333, 0.333, 0.0, 0.333, 0.333, 0.333, 1.0, 1.0, 0.0, 0.667, 0.667, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.333, 0.333, 0.667, 0.0, 1.0, 0.667, 0.0, 0.0, 0.333, 0.333, 0.5, 0.25, 0.0, 0.5, 0.667, 0.333, 0.333, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.667, 0.0, 0.0, 0.0, 0.667, 0.0, 0.667, 0.333, 0.0, 0.0, 0.667, 0.333, 0.0, 0.333, 0.0, 0.0, 0.667, 0.0, 0.0, 0.333, 0.333, 1.0, 0.333, 0.667, 0.333, 0.0, 0.667, 0.333, 1.0, 0.0, 0.667, 0.333, 0.333, 0.0, 0.667, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.667, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.333, 0.333, 0.0, 0.667, 0.0, 0.667, 1.0, 0.333, 0.667, 0.667, 0.667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.667, 0.667, 0.0, 0.333, 0.667, 0.333, 0.333, 0.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.333, 0.333, 0.667, 0.333, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 0.333, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.667, 0.667, 0.667, 0.0, 0.667, 0.333, 1.333, 1.0, 0.333, 0.333, 0.333, 0.333, 0.333, 0.667, 0.333, 0.0, 0.0, 0.333, 1.0, 0.333, 0.25, 0.667, 0.333, 0.0, 0.667, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.667, 0.667, 0.667, 0.667, 0.667, 0.0, 0.667, 0.333, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.5, 0.333, 0.5, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.5, 0.333, 0.667, 0.0, 0.333, 0.333, 1.0, 1.333, 1.0, 0.667, 0.667, 0.667, 0.0, 0.333, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.333, 0.333, 0.0, 0.667, 0.0, 0.667, 0.0, 0.333, 0.333, 0.333, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.333, 0.0, 1.333, 0.667, 0.0, 1.0, 0.333, 0.667, 1.0, 0.667, 0.0, 0.667, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.667, 0.0, 0.0, 1.0, 0.667, 0.667, 0.0, 0.667, 0.0, 0.5, 0.0, 0.667, 0.5, 0.0, 0.0, 0.0, 0.667, 0.333, 0.333, 0.667, 1.0, 1.0, 0.333, 0.333, 1.0, 0.0, 0.0, 0.0, 1.0, 0.667, 1.0, 0.667, 0.333, 0.0, 0.0, 1.0, 0.667, 0.333, 0.5, 0.333, 0.333, 0.333, 1.333, 1.5, 0.5, 0.333, 0.333, 0.667, 0.333, 0.667, 0.0, 0.333, 0.333, 1.0, 0.214, 1.0, 0.333, 0.667, 0.667, 0.667, 0.333, 0.333, 0.333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.667, 0.333, 0.667, 0.333, 0.0, 0.333, 0.0, 0.333, 0.333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.333, 0.5, 1.0, 1.0, 1.0, 0.0, 0.667, 0.0, 1.0, 0.333, 0.0, 0.0, 0.333, 0.333, 0.667, 0.0, 0.667, 0.667, 0.333, 0.333, 0.667, 0.333, 0.667, 0.667, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.5, 0.0, 0.667, 1.0, 0.667, 0.667, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.667, 1.0, 0.0, 0.333, 0.667, 1.0, 0.0, 0.0, 1.0, 0.667, 0.333, 0.667, 0.0, 0.333, 0.5, 1.0, 0.5, 0.0, 1.0, 0.333, 1.0, 0.5, 0.667, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.667, 0.667, 0.0, 0.571, 0.714, 0.667, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.333, 0.0, 0.0, 0.667, 0.333, 0.0, 0.0, 0.333, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.333, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.667, 0.667, 0.333, 0.0, 0.0, 0.333, 0.5, 0.0, 0.333, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.429, 0.857, 0.286, 0.286, 0.0, 0.0, 1.0, 0.667, 0.333, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.333, 0.667, 0.333, 0.0, 0.333, 0.333, 0.333, 0.333, 0.667, 1.0, 0.333, 0.333, 0.333, 0.333, 0.0, 1.0, 0.333, 0.0, 0.0, 0.667, 0.333, 0.0, 0.667, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.667, 0.0, 1.0, 0.333, 0.556, 0.0, 0.0, 0.0, 0.0, 0.75, 0.5, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.333, 0.667, 0.667, 0.333, 0.667, 0.667, 0.333, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.667, 0.667, 0.333, 0.333, 0.333, 0.333, 0.667, 0.667, 0.0, 0.0, 0.333, 0.0, 0.667, 0.667, 0.667, 0.667, 0.667, 0.333, 0.0, 0.667, 0.0, 0.0, 0.0, 0.333, 0.0, 1.333, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.333, 0.0, 0.333, 0.333, 1.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 1.0, 0.667, 0.0, 0.667, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.5, 0.0, 0.333, 0.333, 0.667, 0.667, 0.333, 0.333, 0.667, 0.333, 0.0, 0.0, 0.0, 0.667, 0.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.667, 0.333, 0.333, 0.0, 0.0, 0.667, 1.0, 0.0, 0.333, 0.333, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.667, 0.333, 0.0, 0.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 1.333, 0.0, 1.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.667, 0.333, 0.0, 0.667, 0.333, 0.333, 0.333, 0.0, 0.0, 0.0, 0.667, 0.667, 0.0, 0.667, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.667, 0.0, 0.333, 0.0, 0.667, 0.333, 0.333, 0.667, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 2.667, 0.0, 0.333, 0.0, 0.333, 0.333, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.667, 0.25, 0.0, 0.333, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.667, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 1.0, 0.0, 0.667, 0.0, 0.333, 0.0, 0.667, 0.333, 1.0, 1.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.667, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.667, 0.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.667, 1.0, 0.667, 0.333, 0.333, 0.0, 0.333, 0.333, 0.75, 0.5, 0.667, 0.667, 0.667, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 1.0, 0.0, 0.333, 0.0, 1.0, 0.0, 0.0, 0.333, 0.0, 0.667, 0.0, 0.0, 1.0, 1.2, 0.6, 0.667, 0.333, 0.333, 0.333, 0.333, 0.333, 0.333, 0.333, 0.333, 1.2, 0.667, 1.444, 1.0, 1.5, 0.333, 0.333, 0.333, 0.333, 0.333, 0.667, 0.333, 0.667, 0.333, 0.333, 0.667, 0.333, 1.0, 0.333, 0.667, 1.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.333, 0.667, 1.0, 0.333, 0.0, 0.667, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.667, 0.333, 1.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.333, 0.333, 0.333, 0.333, 0.667, 0.333, 1.0, 0.0, 0.333, 1.0, 0.333, 0.333, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.333, 0.333, 0.667, 0.667, 0.0, 0.0, 0.667, 0.333, 0.0, 0.0, 1.5, 0.5, 0.333, 0.667, 0.667, 0.0, 0.0, 0.0, 0.0, 0.333, 0.5, 1.5, 1.0, 0.0, 0.0, 0.333, 1.5, 0.0, 0.0, 0.333, 0.667, 0.333, 1.0, 0.667, 0.0, 0.667, 0.667, 1.0, 0.5, 1.0, 0.8, 1.0, 0.333, 0.7, 0.0, 0.0, 0.0, 0.667, 0.0, 0.9, 1.0, 1.0, 1.0, 0.75, 0.333, 0.333, 0.667, 0.667, 0.5, 1.0, 0.5, 0.0, 0.333, 1.0, 0.667, 0.333, 1.0, 0.0, 0.2, 0.0, 0.667, 0.333, 0.667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.333, 1.0, 0.0, 0.333, 0.0, 0.333, 0.667, 0.667, 0.333, 1.0, 0.667, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.333, 0.0, 0.333, 0.0, 0.667, 0.333, 0.0, 0.0, 0.333, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 1.0, 0.0, 0.75, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.333, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.75, 0.25, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.667, 0.667, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 1.0, 1.0, 0.667, 0.333, 0.0, 0.0, 0.0, 0.5, 0.0, 0.667, 0.25, 1.333, 0.333, 0.0, 0.0, 0.667, 0.667, 0.333, 0.333, 0.333, 0.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.667, 0.0, 0.6, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.091, 0.909, 0.667, 0.667, 1.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.667, 0.667, 0.333, 0.333, 0.0, 0.0, 0.667, 0.0, 0.0, 0.733, 1.067, 0.333, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.667, 0.0, 0.846, 0.923, 0.5, 0.5, 0.583, 0.667, 0.667, 0.667, 0.333, 0.0, 0.143, 0.429, 0.286, 0.429, 0.0, 0.0, 0.667, 0.333, 0.0, 0.25, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.5, 0.5, 0.75, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.667, 0.0, 0.0, 0.667, 1.0, 1.0, 0.0, 0.0, 0.25, 0.25, 0.25, 0.25, 0.667, 1.0, 1.0, 0.667, 0.667, 0.0, 0.917, 0.75, 0.583, 1.0, 1.0, 1.0, 0.0, 0.0, 0.333, 0.333, 0.667, 0.333, 0.333, 0.667, 0.0, 0.333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.667, 0.667, 0.333, 0.667, 1.0, 0.0, 0.667, 0.667, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.667, 0.0, 0.0, 0.0, 0.667, 0.667, 0.333, 0.0, 1.333, 0.333, 1.333, 1.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.667, 0.0, 0.333, 1.0, 0.667, 0.333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 1.0, 1.333, 1.0, 0.0, 1.0, 0.0, 0.5, 0.667, 1.0, 0.667, 0.333, 1.0, 1.0, 0.667, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.667, 0.667, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.333, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 1.0, 0.5, 0.0, 0.333, 0.333, 1.0, 1.667, 0.333, 0.0, 1.0, 1.667, 0.25, 0.5, 0.0, 0.667, 0.0, 0.0, 0.0, 0.111, 0.333, 0.333, 0.667, 0.0, 0.333, 0.75, 1.0, 0.667, 0.0, 0.667, 0.333, 0.0, 0.667, 0.667, 0.0, 0.333, 0.0, 0.333, 0.333, 0.0, 0.333, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.333, 1.077, 0.0, 1.0, 0.667, 0.0, 0.0, 0.667, 0.0, 0.75, 0.0, 1.0, 0.0, 0.0, 0.333, 0.0, 0.0, 1.0, 0.667, 0.333, 0.308, 1.0, 0.6, 0.538, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.333, 0.0, 0.0, 0.333, 0.0, 0.333, 0.5, 0.5, 0.333, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.667, 0.667, 0.0, 0.0, 0.0, 0.667, 0.333, 0.333, 0.667, 0.0, 0.0, 0.667, 0.0, 0.0, 0.667, 0.0, 0.0, 0.333, 0.333, 0.333, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.333, 0.667, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.455, 0.667, 0.333, 0.667, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.286, 0.286, 0.0, 0.0, 1.0, 0.0, 0.333, 1.0, 0.667, 0.778, 0.556, 0.0, 0.667, 0.667, 1.0, 0.333, 0.0, 0.667, 0.333, 0.667, 0.0, 0.182, 0.333, 0.333, 1.0, 1.0, 0.333, 0.333, 0.7, 0.0, 0.0, 1.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.25, 0.333, 0.625, 1.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.667, 0.0, 0.333, 0.0, 1.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.333, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.667, 0.333, 0.667, 0.5, 0.5, 0.0, 0.25, 0.0, 0.0, 0.5, 0.0, 0.0, 0.7, 0.0, 0.25, 0.0, 0.333, 0.0, 0.667, 0.0, 0.0, 0.333, 0.0, 0.667, 0.333, 0.0, 0.667, 0.667, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 1.0, 0.333, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.25, 0.0, 0.0, 0.333, 0.0, 0.333, 0.667, 1.5, 0.5, 1.333, 0.5, 0.0, 0.667, 1.0, 0.333, 0.667, 0.0, 0.0, 0.5, 0.0, 0.333, 0.333, 0.5, 0.333, 1.0, 1.0, 0.333, 0.333, 0.667, 0.333, 0.667, 0.0, 0.75, 0.5, 0.0, 0.0, 0.333, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.667, 0.333, 0.333, 0.0, 0.667, 0.333, 0.0, 0.0, 0.333, 0.0, 0.0, 0.667, 1.0, 0.5, 0.5, 0.25, 1.0, 0.0, 0.333, 0.667, 0.667, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 1.125, 0.875, 1.0, 0.0, 0.0, 0.75, 0.75, 1.0, 0.5, 0.0, 0.25, 0.0, 0.667, 0.0, 0.667, 0.667, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 1.333, 0.0, 0.0, 0.25, 0.25, 0.0, 0.667, 0.0, 0.0, 0.667, 0.0, 1.0, 1.0, 0.333, 0.0, 0.0, 0.667, 0.0, 0.667, 0.667, 0.0, 0.0, 0.667, 0.333, 1.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.667, 0.0, 0.667, 0.667, 0.333, 0.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.25, 0.333, 0.333, 0.0, 1.5, 0.0, 0.667, 0.333, 0.333, 0.333, 0.0, 0.0, 0.0, 0.667, 1.0, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.333, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 1.0, 1.0, 0.667, 0.667, 0.667, 1.0, 0.333, 1.333, 1.0, 0.333, 0.0, 0.333, 0.333, 0.333, 0.333, 0.0, 1.5, 1.0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6710ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ne_np total no of WER values: 210\n",
      "For merged all total no of WER values: 11027\n"
     ]
    }
   ],
   "source": [
    "print(\"For ne_np total no of WER values:\",len(ne_np_female))\n",
    "print(\"For merged all total no of WER values:\",len(merged_all))\n",
    "x=ne_np_female\n",
    "y=merged_all\n",
    "# Avg_WER=sum(weer_list)/len(weer_list)\n",
    "# print(Avg_WER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a75645e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AvgWer_female: 0.5375714285714288\n",
      "AvgWer_merged: 0.412978597986759\n"
     ]
    }
   ],
   "source": [
    "Avg_WER_female=sum(x)/len(x)\n",
    "Avg_WER_merged=sum(y)/len(y)\n",
    "print(\"AvgWer_female:\",Avg_WER_female)\n",
    "print(\"AvgWer_merged:\",Avg_WER_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a51a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[1,2,3,4,5,6,7]\n",
    "y=[]\n",
    "for i in range(2,5):\n",
    "    y.append(i)\n",
    "z=x[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "937b8b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "[3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1b89cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myname\n",
      "\n",
      "\n",
      "1213\n"
     ]
    }
   ],
   "source": [
    "shi=\"myname:1213\"\n",
    "x=shi.split(\":\")\n",
    "print(x[0])\n",
    "print(\"\\n\")\n",
    "print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971641b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
